{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"theory/arima/","title":"ARIMA \u2014 Theory in Practice","text":"<p>ARIMA is the canonical model for short-horizon forecasting when a single series exhibits short-memory dynamics. It marries three ideas:</p> <ol> <li>Autoregression (AR): today echoes past levels.</li> <li>Moving Average (MA): today also echoes past shocks.</li> <li>Integration (I): if levels drift, difference them until the mean is stable.</li> </ol> <p>Formally, an ARIMA(p, d, q) for a series \\(x_t\\) applies \\(d\\) differences and then fits an ARMA on \\(\\Delta^d x_t\\):</p> \\[ \\phi(B)\\,\\Delta^d x_t = \\theta(B)\\,\\varepsilon_t,\\qquad \\varepsilon_t \\stackrel{i.i.d.}{\\sim} (0,\\sigma^2), \\] <p>with \\(\\phi(B)=1-\\phi_1 B-\\cdots-\\phi_p B^p\\) and \\(\\theta(B)=1+\\theta_1 B+\\cdots+\\theta_q B^q\\). Here \\(B\\) is the backshift operator, \\(B x_t = x_{t-1}\\).</p>"},{"location":"theory/arima/#from-arma-to-arima-why-differencing-matters","title":"From ARMA to ARIMA (why differencing matters)","text":"<p>A Classical ARMA models assume the series is weakly stationary\u2014constant mean/variance and an autocovariance that depends only on the lag. Many economic and financial series aren\u2019t: they drift, trend, or exhibit unit roots. Differencing is the bridge from a non-stationary data-generating process to a stationary one that ARMA can legitimately model. That\u2019s the \u201cI\u201d in ARIMA.</p>"},{"location":"theory/arima/#what-differencing-does-mathematically-and-intuitively","title":"What differencing does (mathematically and intuitively)","text":"<p>The first difference operator is</p> \\[ \\nabla x_t = x_t - x_{t-1} = (1 - B)\\,x_t, \\] <p>with \\(B\\) the backshift operator. Applying it \\(d\\) times yields</p> \\[ \\nabla^d x_t = (1 - B)^d\\,x_t. \\] <ul> <li>If \\(x_t\\) contains a unit root (a root at \\(z=1\\) of the AR polynomial), then \\(\\nabla x_t\\) removes that root and typically produces a stationary series.</li> <li>Intuition: differencing converts levels to changes. Deterministic trends become approximately constant shifts; stochastic trends (random walks) become mean-reverting innovations.</li> </ul> <p>When we say ARIMA\\((p,d,q)\\), we mean that \\(y_t = \\nabla^d x_t\\) is well-described by an ARMA\\((p,q)\\):</p> \\[ \\phi(B)\\,y_t = \\theta(B)\\,\\varepsilon_t,\\qquad \\varepsilon_t \\sim \\text{i.i.d. }(0,\\sigma^2). \\] <p>We model \\(y_t\\) (the differenced series) with ARMA, then integrate back (cumulate the forecasts) to obtain predictions for \\(x_t\\).</p>"},{"location":"theory/arima/#how-many-differences-do-we-need","title":"How many differences do we need?","text":"<p>There is no prize for over-differencing. We aim for the smallest \\(d\\) that yields an approximately stationary series:</p> <ul> <li>\\(d=0\\): already stationary in level (common for returns, log-returns).</li> <li>\\(d=1\\): removes a single unit root (random-walk-like behavior).</li> <li>\\(d=2\\): rarely necessary; implies acceleration in the level process (e.g., integrated of order two).</li> </ul> <p>Practical decision aids:</p> <ul> <li>Visual: level series with pronounced drift and ACF that decays very slowly suggests differencing; the first difference should look mean-reverting with a rapidly decaying ACF.</li> <li>Tests: unit-root tests (ADF) and stationarity checks (KPSS) provide complementary evidence. (We develop the formalities in the Statistical Concepts chapter.)</li> <li>Residual diagnostics after provisional fits: if ARMA on the differenced series still shows low-frequency leakage (residual ACF significant at large lags), you may be under-differenced.</li> </ul>"},{"location":"theory/arima/#the-cost-of-differencing-variance-and-signal","title":"The cost of differencing (variance and signal)","text":"<p>Differencing is not free:</p> <ul> <li>It amplifies high-frequency noise. For white noise \\(w_t\\), \\(\\nabla w_t = w_t - w_{t-1}\\) has variance \\(2\\sigma^2\\).</li> <li>It removes low-frequency content that may be predictive (e.g., smooth trends).</li> <li>It reduces effective sample size for estimation and forecasting at the level scale (integration accumulates uncertainty).</li> </ul> <p>Hence the \u201cminimal \\(d\\)\u201d principle: take just enough differences to achieve stationarity and push the remaining structure into \\(p\\) and \\(q\\).</p>"},{"location":"theory/arima/#over-differencing-vs-under-differencing","title":"Over-differencing vs under-differencing","text":"<ul> <li>Under-differenced: residual ACF decays slowly; information criteria push toward larger \\(p\\) to mimic the trend; forecasts may drift spuriously.</li> <li>Over-differenced: residuals show negative autocorrelation at lag 1; the MA part balloons to compensate; forecast intervals widen unnecessarily.</li> </ul> <p>A quick heuristic: if the ACF of \\(\\nabla x_t\\) is strongly negative at lag 1 and otherwise small, you may have over-differenced.</p>"},{"location":"theory/arima/#drift-and-deterministic-components","title":"Drift and deterministic components","text":"<p>With \\(d=1\\), a drift term in the differenced model is equivalent to a linear trend in levels. In ARIMA notation this is the constant/trend handling:</p> <ul> <li>For \\(d=0\\), a constant (trend = 'c') produces a non-zero mean stationary process.</li> <li>For \\(d\\ge 1\\), including a constant behaves like a polynomial trend in levels; many implementations disable lower-order trends automatically to avoid identification issues.</li> </ul> <p>AutoArimaExplorer mirrors this: for \\(d=0\\) it tries with/without constant; for \\(d\\ge 1\\) it toggles drift sensibly while respecting identifiability.</p>"},{"location":"theory/arima/#seasonal-differencing-briefly","title":"Seasonal differencing (briefly)","text":"<p>If there is seasonal integration\u2014e.g., a unit root at the seasonal frequency\u2014apply seasonal differencing \\(\\nabla_s x_t = x_t - x_{t-s}\\) possibly in addition to the non-seasonal \\(\\nabla\\). Purely non-seasonal ARIMA often mis-specifies such series, leading to stubborn seasonal autocorrelation in residuals. (We treat SARIMA in a dedicated chapter.)</p>"},{"location":"theory/arima/#how-differencing-interacts-with-p-and-q","title":"How differencing interacts with \\(p\\) and \\(q\\)","text":"<p>Differencing reshapes the correlation structure:</p> <ul> <li>Many stochastic trends can be captured either by higher \\(p\\) in ARMA or by one difference plus smaller \\(p\\). The latter is usually cleaner and more stable.</li> <li>After differencing, expect short-memory residual dynamics; \\(p\\) and \\(q\\) should be modest (e.g., 0\u20133). If you keep needing large \\(p\\)/\\(q\\), revisit \\(d\\) (or seasonality, exogenous drivers, or variance dynamics).</li> </ul>"},{"location":"theory/arima/#common-data-issues-and-what-differencing-cant-fix","title":"Common data issues (and what differencing can\u2019t fix)","text":"<ul> <li>Structural breaks / regime shifts: differencing doesn\u2019t heal breaks; consider break detection, dummy variables, or regime-switching models.</li> <li>Heteroskedasticity (volatility clustering): differencing mean structure doesn\u2019t remove conditional variance dynamics; consider GARCH-type or robust errors. (Our residual squared Ljung\u2013Box check is a quick proxy.)</li> <li>Nonlinearities: if dynamics depend on the state (thresholds, saturations), linear ARIMA may be systematically biased.</li> <li>Strong seasonality or calendar effects: handle seasonality explicitly; differencing alone is crude.</li> </ul>"},{"location":"theory/arima/#practical-diagnostics-youll-see-in-this-project","title":"Practical diagnostics you\u2019ll see in this project","text":"<p>AutoArimaExplorer enforces a few disciplined checks:</p> <ul> <li>Ljung\u2013Box on residuals (and optionally on squared residuals): if differencing and ARMA are doing their job, residual autocorrelation\u2014especially at low lags\u2014should be statistically negligible.</li> <li>Information criteria (AIC/BIC): used to balance fit and parsimony on the differenced series.</li> <li>Competing \\(d\\): we explicitly compare \\(d \\in \\{0,1,2\\}\\), letting diagnostics, not habit, choose the level of integration.</li> </ul>"},{"location":"theory/arima/#bottom-line","title":"Bottom line","text":"<p>ARIMA is not \u201cARMA but fancier\u201d\u2014it\u2019s ARMA on the right scale. Correct differencing is the decisive step that turns drifting, unit-rooty level data into a stable signal where short-memory ARMA can do honest work. Get \\(d\\) right, and \\(p\\) and \\(q\\) become small, interpretable, and forecast-useful; get \\(d\\) wrong, and the rest is damage control.</p>"},{"location":"theory/arima/#stationarity-invertibility-the-root-picture","title":"Stationarity &amp; Invertibility (the root picture)","text":"<p>ARIMA models live or die by the location of a few complex numbers. The most useful way to reason about stationarity (for the AR part) and invertibility (for the MA part) is to look at the roots of their characteristic polynomials in the complex plane.</p>"},{"location":"theory/arima/#the-two-polynomials","title":"The two polynomials","text":"<p>For an ARMA\\((p,q)\\) model on a (differenced) series \\(y_t\\),</p> \\[ \\phi(B)\\,y_t \\;=\\; \\theta(B)\\,\\varepsilon_t,\\qquad \\varepsilon_t\\sim\\text{i.i.d. }(0,\\sigma^2), \\] <p>with</p> \\[ \\phi(B)=1-\\phi_1 B-\\cdots-\\phi_p B^p,\\qquad \\theta(B)=1+\\theta_1 B+\\cdots+\\theta_q B^q. \\] <ul> <li>AR stationarity: All roots of \\(\\phi(z)=0\\) must lie outside the unit circle \\(\\{|z|&gt;1\\}\\).</li> <li>MA invertibility: All roots of \\(\\theta(z)=0\\) must lie outside the unit circle \\(\\{|z|&gt;1\\}\\).</li> </ul> <p>These conditions guarantee:</p> <ul> <li>A stable moving-average representation \\(y_t=\\sum_{k\\ge 0}\\psi_k \\varepsilon_{t-k}\\) with absolutely summable \\(\\{\\psi_k\\}\\),</li> <li>A unique innovation process (no observationally equivalent \u201cnon-invertible\u201d parameterizations).</li> </ul> <p>In ARIMA\\((p,d,q)\\), the differenced series \\(y_t=\\nabla^d x_t\\) should satisfy the ARMA root conditions above. The unit roots associated with \\((1-B)^d\\) live on the unit circle by design\u2014that\u2019s the \u201cI\u201d you chose to difference away.</p>"},{"location":"theory/arima/#quick-intuition-with-ar1-and-ma1","title":"Quick intuition with AR(1) and MA(1)","text":"<ul> <li>AR(1): \\(y_t=\\phi y_{t-1}+\\varepsilon_t\\).   \\(\\phi(z)=1-\\phi z\\Rightarrow z=1/\\phi\\). Stationary iff \\(|\\phi|&lt;1\\) (root at \\(|z|&gt;1\\)).</li> <li>MA(1): \\(y_t=\\varepsilon_t+\\theta \\varepsilon_{t-1}\\).   \\(\\theta(z)=1+\\theta z\\Rightarrow z=-1/\\theta\\). Invertible iff \\(|\\theta|&lt;1\\).</li> </ul> <p>A useful mental model: the closer a root is to the unit circle, the more persistent (slowly decaying) the correlation pattern. Cross the circle and persistence becomes explosive (AR) or the innovations become non-unique (MA).</p>"},{"location":"theory/arima/#the-geometry-what-roots-mean","title":"The geometry: what roots \u201cmean\u201d","text":"<ul> <li>Real roots \\(r&gt;1\\) (AR): slow, monotone decay in the ACF (persistence).   Roots near \\(1^+\\) \u2194 very long memory in mean.</li> <li>Real roots \\(r&lt;-1\\) (AR): alternating-sign decay (oscillations with period \\~2).</li> <li>Complex conjugate roots \\(re^{\\pm i\\omega}\\) (AR): damped cycles with angular frequency \\(\\omega\\) and damping \\(r^{-k}\\). Period \\(\\approx 2\\pi/\\omega\\).</li> <li>MA roots control invertibility and the shape of the ACF at small lags; non-invertible parameterizations have an equivalent invertible twin (flip a root \\(z\\) inside the unit circle to its reciprocal \\(1/\\bar z\\)) producing the same second-order behavior but a different innovation sequence. That\u2019s why we enforce invertibility: it\u2019s a normalization for uniqueness.</li> </ul>"},{"location":"theory/arima/#why-roots-near-1-are-troublesome","title":"Why roots near 1 are troublesome","text":"<p>Roots that hug the unit circle cause:</p> <ul> <li>Numerical fragility (flat likelihood; optimizer plateaus).</li> <li>Huge forecast persistence (wide intervals, slow mean reversion).</li> <li>Diagnostic confusion: residual autocorrelation decays so slowly that Ljung\u2013Box rejects unless you difference or simplify.</li> </ul> <p>Practically, if an AR root is very close to \\(1\\) (or a seasonal root to \\(e^{2\\pi i/s}\\)), you probably need (seasonal) differencing rather than higher-order AR terms.</p>"},{"location":"theory/arima/#arima-and-unit-roots","title":"ARIMA and unit roots","text":"<p>ARIMA explicitly factors unit roots into \\((1-B)^d\\). The stationarity condition applies to the remaining AR polynomial \\(\\phi\\). Equivalently:</p> \\[ (1-B)^d\\,\\phi(B)\\,x_t=\\theta(B)\\,\\varepsilon_t,\\quad \\text{with }\\phi(z)\\text{ root-outside and }\\theta(z)\\text{ root-outside.} \\] <p>So the differenced process is stationary/invertible, while the level process has \\(d\\) unit roots by construction.</p>"},{"location":"theory/arima/#companion-matrix-view-for-the-linear-algebra-inclined","title":"Companion matrix view (for the linear-algebra inclined)","text":"<p>Let \\(A\\) be the AR companion matrix. Stationarity is equivalent to the spectral radius \\(\\rho(A)&lt;1\\). This is the same \u201croot outside the unit circle\u201d condition in matrix clothing and explains why near-unit roots create near-unit eigenvalues\u2014ill-conditioned likelihoods and fragile estimates.</p>"},{"location":"theory/arima/#practical-diagnostics-safeguards-youll-see","title":"Practical diagnostics &amp; safeguards you\u2019ll see","text":"<ul> <li>Root checks: It\u2019s good practice to compute AR &amp; MA roots post-fit and confirm \\(|z|&gt;1\\) with margin. Roots with \\(|z|\\approx 1\\) merit simpler models or differencing.</li> <li>Constraints in fitting: Many libraries (including what we leverage) can enforce stationarity/invertibility during optimization. AutoArimaExplorer tries both constrained and unconstrained trials and keeps the best admissible fit.</li> <li>Ljung\u2013Box (residuals &amp; squared residuals): If roots are well-behaved, residual autocorrelation should be negligible. Persistent low-lag structure often points to roots too close to 1 (or missing seasonality/exogenous drivers).</li> </ul>"},{"location":"theory/arima/#common-patterns-to-recognize","title":"Common patterns to recognize","text":"<ul> <li>Under-differenced: AR roots estimated very near 1; slow ACF tail; models prefer large \\(p\\). Fix with differencing.</li> <li>Over-differenced: Strong negative lag-1 ACF; MA balloons to compensate. Reduce \\(d\\).</li> <li>Non-invertible MA: Coefficients imply \\(|z|\\le 1\\) for some MA root; flip to the invertible representation (libraries usually do this or we reject such trials).</li> </ul> <p>For formal definitions of weak stationarity, Wold decomposition, absolute summability, and asymptotic properties, see the Statistical Concepts chapter. Here, the rule of thumb is simple: keep all AR and MA roots comfortably outside the unit circle, and your ARIMA will behave like a stable linear filter rather than a barely-tamed random walk.</p>"},{"location":"theory/arima/#the-role-of-constants-trends-and-drift","title":"The role of constants, trends, and drift","text":"<p>A surprisingly large share of ARIMA modeling comes down to a small choice: do we include a deterministic level or slope? In practice this shows up as a constant (intercept), a linear trend, or, after differencing, a drift. Getting this right determines long-run behavior and the shape of your forecasts.</p>"},{"location":"theory/arima/#arma-constants-set-the-long-run-mean","title":"ARMA: constants set the long-run mean","text":"<p>For a stationary ARMA\\((p,q)\\) model on \\(y_t\\),</p> \\[ y_t \\;=\\; c \\;+\\; \\sum_{i=1}^{p}\\phi_i\\,y_{t-i} \\;+\\; \\varepsilon_t \\;+\\; \\sum_{j=1}^{q}\\theta_j\\,\\varepsilon_{t-j}, \\quad \\varepsilon_t\\stackrel{\\text{i.i.d.}}{\\sim}(0,\\sigma^2), \\] <p>the unconditional mean exists and equals</p> \\[ \\mu \\;=\\; \\frac{c}{\\,1-\\sum_{i=1}^{p}\\phi_i\\,}. \\] <ul> <li>If you omit \\(c\\) (trend='n'), you force \\(\\mu=0\\). That\u2019s often appropriate for returns.</li> <li>If you include \\(c\\) (trend='c'), the process mean-reverts to \\(\\mu\\), and multi-step forecasts flatten toward \\(\\mu\\).</li> </ul> <p>Heuristic: For \\(d=0\\), use no constant when working with demeaned data or near-zero-mean returns; allow a constant at levels if a nonzero mean is plausible.</p>"},{"location":"theory/arima/#from-arma-to-arima-why-drift-appears","title":"From ARMA to ARIMA: why \u201cdrift\u201d appears","text":"<p>In ARIMA\\((p,d,q)\\) we model \\(x_t\\) through \\(d\\)-th differences \\(y_t = \\nabla^{d} x_t\\) that follow ARMA\\((p,q)\\). Deterministic terms transform under differencing:</p> <ul> <li>A constant in levels disappears after one difference.</li> <li>A linear trend in levels becomes a constant (drift) after one difference.</li> <li>In general, a polynomial trend of degree \\(m\\) in levels becomes a polynomial of degree \\(m-d\\) (or vanishes if \\(m&lt;d\\)) in the differenced series.</li> </ul> <p>For the common \\(d=1\\) case (random-walk-like levels), you\u2019ll see drift:</p> \\[ \\nabla x_t \\equiv x_t - x_{t-1} \\;=\\; \\delta \\;+\\; \\text{ARMA}(p,q)\\text{ noise}. \\] <p>Equivalently, \\(x_t\\) is a random walk with drift \\(\\delta\\) plus stationary noise. Its expectation is linear:</p> \\[ \\mathbb{E}[x_t] \\;=\\; x_0 \\;+\\; \\delta\\,t. \\] <p>Forecasts inherit a linear slope \\(\\delta\\): long-horizon point forecasts march upward (or downward) at rate \\(\\delta\\), while uncertainty widens with horizon.</p>"},{"location":"theory/arima/#what-your-softwares-trend-flag-really-means","title":"What your software\u2019s \u201ctrend\u201d flag really means","text":"<p>Most libraries expose deterministic terms via a <code>trend</code> argument:</p> <ul> <li><code>trend='n'</code> \u2014 no deterministic term.</li> <li><code>trend='c'</code> \u2014 constant (intercept).</li> <li><code>trend='t'</code> \u2014 linear time trend.</li> <li><code>trend='ct'</code> \u2014 both constant and trend (when identifiable).</li> </ul> <p>Crucial constraint (identifiability): When you difference \\(d\\) times, trend terms of order &lt; \\(d\\) are not identifiable (they are annihilated by differencing) and should be omitted. Concretely:</p> <ul> <li>With \\(d=0\\): you may use <code>'n'</code>, <code>'c'</code>, <code>'t'</code>, or <code>'ct'</code> as appropriate.</li> <li>With \\(d=1\\): a pure constant in levels vanishes, but a linear trend in levels corresponds to drift in \\(\\nabla x_t\\). Use <code>'t'</code> (not <code>'c'</code>). Many libraries will warn or silently drop invalid terms.</li> <li>With \\(d\\ge 2\\): only higher-order trends survive; in most practical ARIMA work, you set <code>'n'</code> unless you explicitly model higher-order polynomials.</li> </ul> <p>Mapping tip: \u201cdrift\u201d in the differenced equation is equivalent to selecting a linear trend in levels (e.g., <code>trend='t'</code> when \\(d=1\\)). A plain intercept (<code>'c'</code>) is appropriate for \\(d=0\\) but not for \\(d=1\\).</p>"},{"location":"theory/arima/#forecast-behavior-mean-reversion-vs-linear-growth","title":"Forecast behavior: mean reversion vs. linear growth","text":"<ul> <li>ARMA with constant (d=0): forecasts revert to \\(\\mu\\).</li> <li>ARIMA with drift (d=1): forecasts grow linearly with slope \\(\\delta\\).</li> <li>No deterministic term: forecasts revert to zero (d=0) or to the last level (random walk without drift, \\(d=1\\)).</li> </ul> <p>Choose the specification that matches domain knowledge:</p> <ul> <li>Financial prices \u2192 often \\(d=1\\) with no drift unless there\u2019s structural growth.</li> <li>Macroeconomic levels (e.g., GDP) \u2192 \\(d=1\\) and drift is common.</li> <li>Stationary spreads/returns \u2192 \\(d=0\\) with or without a constant depending on the mean.</li> </ul>"},{"location":"theory/arima/#common-pitfalls-and-how-to-spot-them","title":"Common pitfalls and how to spot them","text":"<ul> <li>Including an intercept when \\(d=1\\): it\u2019s not identified; your library will warn or drop it. Use <code>'t'</code> (drift) instead if a slope is justified.</li> <li>Spurious trends: an apparent slope driven by a short sample or regime shift. Check stability (rolling estimates) and use out-of-sample validation.</li> <li>Over-deterministic modeling: a strong linear trend plus AR terms can overfit; let differencing and AR capture persistence unless a trend is truly structural.</li> <li>Forecast pathology: If long-horizon forecasts explode or hug the last observation implausibly, revisit the deterministic term: you may need to add/drop drift or switch between <code>'c'</code> and <code>'n'</code>.</li> </ul>"},{"location":"theory/arima/#quick-decision-guide","title":"Quick decision guide","text":"<ol> <li> <p>Is the working series stationary (after your chosen differencing)?</p> <ul> <li>Yes (\\(d=0\\)) \u2192 Consider a constant only if the mean is materially nonzero.</li> <li>No (\\(d=1\\)) \u2192 Consider drift (via <code>trend='t'</code>) if a linear trend in levels is credible.</li> </ul> </li> <li> <p>Do forecasts need a long-run level or slope?</p> <ul> <li>Level (mean reversion) \u2192 constant at \\(d=0\\).</li> <li>Slope (steady growth/decline) \u2192 drift at \\(d=1\\).</li> </ul> </li> <li> <p>Do diagnostics agree?</p> <ul> <li>Intercept/drift significant; BIC improves; residuals pass Ljung\u2013Box \u2192 keep it.</li> <li>Otherwise, prefer the simpler deterministic structure.</li> </ul> </li> </ol>"},{"location":"theory/arima/#forecasts-and-uncertainty","title":"Forecasts and uncertainty","text":"<p>Forecasting with ARIMA is more than a point prediction. We also need a distribution for the future, because decisions hinge on how wide the plausible range is. This section lays out (i) how ARIMA point forecasts are formed, (ii) how forecast errors accumulate with the horizon, and (iii) how to build prediction intervals (PIs)\u2014including the caveats that matter in practice.</p>"},{"location":"theory/arima/#point-forecasts-via-the-ma-view","title":"Point forecasts via the MA(\u221e) view","text":"<p>Any causal ARMA\\((p,q)\\) (and thus any ARIMA once you difference) admits an MA(\u221e) representation</p> \\[ y_t \\;=\\; \\mu + \\sum_{j=0}^{\\infty}\\psi_j \\,\\varepsilon_{t-j},  \\qquad \\psi_0=1,\\quad \\sum_j|\\psi_j|&lt;\\infty, \\] <p>with \\(\\varepsilon_t \\overset{i.i.d.}{\\sim}(0,\\sigma^2)\\) and \\(\\psi(B)=\\frac{\\theta(B)}{\\phi(B)}\\). For horizon \\(h\\ge 1\\),</p> \\[ \\widehat{y}_{t}(h) \\;\\equiv\\; \\mathbb{E}[y_{t+h}\\mid \\mathcal{F}_t] \\;=\\; \\mu + \\sum_{j=h}^{\\infty}\\psi_j \\,\\varepsilon_{t+h-j}, \\] <p>since future shocks (\\(j&lt;h\\)) have zero mean. In practice, software computes \\(\\widehat{y}_t(h)\\) by recursion in the ARIMA state or by using the MA(\u221e) weights; both yield the same predictor.</p> <ul> <li>ARMA (stationary): forecasts mean revert to \\(\\mu\\).</li> <li>ARIMA with \\(d=1\\): if the differenced series has a drift \\(\\delta\\), levels follow a random-walk-like path and point forecasts grow linearly: \\(\\widehat{x}_{t}(h) \\approx x_t + \\delta\\,h\\) (plus ARMA adjustments).</li> </ul>"},{"location":"theory/arima/#forecast-error-variance-why-bands-widen","title":"Forecast error variance (why bands widen)","text":"<p>Define the \\(h\\)-step forecast error \\(e_t(h) = y_{t+h}-\\widehat{y}_t(h)\\). For ARMA,</p> \\[ e_t(h) \\;=\\; \\sum_{j=0}^{h-1}\\psi_j\\,\\varepsilon_{t+h-j}, \\qquad \\operatorname{Var}[e_t(h)] \\;=\\; \\sigma^2 \\sum_{j=0}^{h-1}\\psi_j^2. \\] <p>Key implications:</p> <ul> <li>The one-step error variance is \\(\\sigma^2\\).</li> <li>As \\(h\\) grows, the partial sum \\(\\sum_{j=0}^{h-1}\\psi_j^2\\) increases, so intervals widen.</li> <li> <p>For ARIMA with differencing (\\(d\\ge 1\\)), the variance of levels grows without bound:</p> </li> <li> <p>Random walk (\\(d=1\\), no drift): \\(\\operatorname{Var}[x_{t}(h)-x_t] = \\sigma^2 h\\).</p> </li> <li>Random walk with drift: same variance; drift affects the mean, not the spread.</li> </ul> <p>More generally, if \\(\\psi(B) = \\theta(B)/\\phi(B)\\) and the roots of \\(\\phi(z)\\) are outside the unit circle (stationarity), \\(\\psi_j\\) is absolutely summable and \\(\\sum_{j=0}^{h-1}\\psi_j^2\\) stabilizes as \\(h\\to\\infty\\) for ARMA. With integration (\\(d\\ge 1\\)), differencing introduces an additional accumulation that pushes level-forecast variance upward with \\(h\\).</p>"},{"location":"theory/arima/#prediction-intervals-pis-vs-confidence-intervals-cis","title":"Prediction intervals (PIs) vs. confidence intervals (CIs)","text":"<ul> <li>A prediction interval aims to cover the future observation, not the (unknown) mean. Under Gaussian innovations,</li> </ul> <p>$$   \\text{PI}{1-\\alpha}(h):\\quad \\widehat{y}_t(h) \\;\\pm\\; z{1-\\alpha/2}\\,\\sqrt{\\widehat{\\operatorname{Var}}[e_t(h)]},   $$</p> <p>where \\(\\widehat{\\operatorname{Var}}[e_t(h)]\\) comes from the model and \\(z_{1-\\alpha/2}\\) is the standard normal quantile. * A confidence interval for the forecast mean is narrower; it ignores the one-step-ahead innovation term and (in ARIMA) is rarely what you want for decisions.</p> <p>Coverage caveat. Standard PIs typically ignore parameter uncertainty (estimation error in \\(\\phi,\\theta,\\sigma^2\\)). With short samples or highly persistent dynamics, nominal 95% PIs can under-cover. Two remedies:</p> <ol> <li>Simulation (parametric bootstrap): simulate many future paths using the estimated model; take empirical quantiles at each horizon.</li> <li>Asymptotic adjustment: add a term derived from the delta method and the parameter covariance; rarely implemented in general ARIMA stacks.</li> </ol>"},{"location":"theory/arima/#drift-and-long-horizon-behavior","title":"Drift and long-horizon behavior","text":"<p>For \\(d=1\\) with drift \\(\\delta\\), level forecasts step forward at slope \\(\\delta\\):</p> \\[ \\widehat{x}_t(h) \\;=\\; x_t + \\delta\\,h + \\text{ARMA corrections}. \\] <p>Uncertainty is driven by the integrated noise, so the fan chart widens roughly like \\(\\sqrt{h}\\) for a random walk. If you see bands that do not widen in \\(d=1\\) levels, you are likely looking at intervals for differences rather than levels, or a misreported variance.</p>"},{"location":"theory/arima/#non-gaussian-and-heteroskedastic-shocks","title":"Non-Gaussian and heteroskedastic shocks","text":"<p>PIs above assume i.i.d. Gaussian innovations with constant variance. Real data often deviate:</p> <ul> <li>Heavy tails / outliers: the normal-quantile recipe leads to undercoverage. Use robust quantiles via simulation or fit a heavier-tailed noise.</li> <li> <p>Conditional heteroskedasticity (ARCH/GARCH): variance evolves over time, so \\(\\operatorname{Var}[e_t(h)]\\) depends on the current volatility state. Remedies:</p> </li> <li> <p>Model volatility explicitly (ARIMA-GARCH).</p> </li> <li>Or transform to stabilize variance (e.g., log-returns), then model.</li> </ul>"},{"location":"theory/arima/#transforms-and-back-transforms","title":"Transforms and back-transforms","text":"<p>If you model a transformed series \\(g(x_t)\\) (logs, Box\u2013Cox, differencing), take care when back-transforming forecasts and intervals:</p> <ul> <li>Log scale: if \\(y_t=\\log x_t\\) and \\(\\widehat{y}_t(h)\\sim \\mathcal{N}(m,s^2)\\), the unbiased mean on the original scale is \\(\\exp(m+\\tfrac12 s^2)\\) (log-normal correction). Applying \\(\\exp\\) naively to the endpoints of a symmetric PI on log scale yields an asymmetric PI on the original scale\u2014this is correct and expected.</li> <li>Differencing: level forecasts are obtained by integrating the predicted differences; the variance aggregates across steps, hence wider bands.</li> </ul>"},{"location":"theory/arima/#practical-checklist","title":"Practical checklist","text":"<ol> <li>Pick the horizon: Decide whether you need PIs for differences (flows) or levels (stocks).</li> <li>Use the right deterministic term: drift in \\(d=1\\) if a linear trend in levels is plausible; otherwise omit it.</li> <li>Report PIs, not just points: Use model-based variance; for short samples or high persistence, prefer simulation-based bands.</li> <li>Stress test coverage: Backtest PIs; aim for nominal coverage (e.g., \\~95%). Tools like the Winkler score or empirical coverage by horizon help quantify calibration.</li> <li>Mind volatility: If residuals show ARCH effects, expect undercoverage with homoskedastic ARIMA; consider ARIMA-GARCH or variance-stabilizing transforms.</li> <li>Communicate widening: Stakeholders should expect wider uncertainty at longer horizons\u2014especially for integrated processes.</li> </ol> <p>This treatment stays model-based and distribution-aware. For hypothesis tests, residual diagnostics, information criteria, and formal coverage backtests, see the Statistical Concepts chapter.</p>"},{"location":"theory/arima/#when-arima-works-best","title":"When ARIMA works best","text":"<p>ARIMA shines when your series is well-explained by linear, short-memory dynamics after a small amount of differencing. In that regime, it delivers sharp short-to-medium-term forecasts, interpretable parameters, and well-calibrated uncertainty\u2014without the overhead of complex models.</p>"},{"location":"theory/arima/#the-sweet-spot-data-properties","title":"The sweet spot (data properties)","text":"<p>ARIMA is a strong choice when most of the following hold:</p> <ul> <li>Regular sampling &amp; enough history. Observations at a fixed cadence (no big gaps), with at least \\~100\u2013200 points for stable estimation and diagnostics.</li> <li>Low-order integration. One difference (occasionally two) stabilizes level/variance and removes obvious trends; differenced series looks roughly stationary.</li> <li>Short-range dependence. Autocorrelations decay quickly; PACF/ACF suggest low orders (small \\(p,q\\)).</li> <li>Variance is stable (or can be stabilized). Raw scale is homoskedastic, or becomes so after a simple transform (log, Box\u2013Cox).</li> <li>No strong seasonality (or it\u2019s handled explicitly with SARIMA). The nonseasonal ARIMA part is for the \u201cwithin-season\u201d dynamics.</li> <li>No frequent regime shifts. Mean/variance don\u2019t jump around due to structural breaks; interventions are infrequent and can be modeled separately.</li> <li>Innovations not too wild. Residuals are roughly Gaussian with thin/moderate tails; outliers are occasional rather than the rule.</li> </ul> <p>When these ingredients line up, ARIMA tends to produce forecasts that are hard to beat with much heavier machinery.</p>"},{"location":"theory/arima/#typical-winning-use-cases","title":"Typical winning use cases","text":"<ul> <li>Differenced levels with drift. Economic indicators, demand/consumption, web traffic, or price levels where \\(\\nabla x_t\\) behaves like a weakly-dependent process and a drift captures the average trend.</li> <li>Return-like series. Financial/crypto log-returns: mean \\~0, short memory, good fit with ARMA (often \\(p,q \\le 2\\)). If volatility clusters strongly, consider ARIMA-GARCH.</li> <li>Operational short-term demand. Call volumes, ticket arrivals, inventory withdrawals\u2014once detrended (and deseasonalized if needed), ARIMA captures day-to-day dynamics.</li> <li>Post-processing of ML forecasts. ARIMA on residuals of a baseline model (hybrid ARIMA-X) to mop up linear autocorrelation left on the table.</li> </ul>"},{"location":"theory/arima/#signals-arima-is-a-good-idea-quick-checklist","title":"Signals ARIMA is a good idea (quick checklist)","text":"<ul> <li>After differencing: ACF tails off quickly; PACF has a few significant spikes only.</li> <li>Diagnostics: Residuals pass Ljung\u2013Box at standard lags; no obvious structure in residual ACF/PACF; variance looks flat over time.</li> <li>Parsimony: Small \\(p,q\\) give you most of the performance; adding lags doesn\u2019t materially reduce BIC or OOS error.</li> <li>Stable parameters: Re-fits on rolling windows yield similar coefficients and forecast error profiles.</li> </ul> <p>Tip: In AutoArimaExplorer, these checks are automated via BIC selection, Ljung\u2013Box guardrails, and optional out-of-sample (rolling) validation.</p>"},{"location":"theory/arima/#where-arima-is-not-your-first-pick-and-what-to-do","title":"Where ARIMA is not your first pick (and what to do)","text":"<ul> <li>Strong seasonality or calendar effects. Use SARIMA (seasonal terms), or decompose first (STL, calendar regressors) and ARIMA the remainder.</li> <li>Time-varying volatility. Clear ARCH/GARCH signatures \u2192 consider ARIMA-GARCH or variance-stabilizing transforms.</li> <li>Nonlinear dynamics / regime changes. Markov-switching, threshold effects, structural breaks \u2192 look at state-switching models or tree/NN methods with change-point handling.</li> <li>Exogenous drivers dominate. If weather, price, promotions, or macro covariates explain most variance, use ARIMAX (ARIMA with regressors) or a pure regression with autocorrelated errors.</li> <li>Sparse/irregular sampling. Consider state-space models with missing-data handling, or continuous-time alternatives.</li> <li>Very long horizons. Integrated processes accumulate uncertainty fast; for long-range planning, pair ARIMA with scenario analysis or hierarchical/aggregated signals.</li> </ul>"},{"location":"theory/arima/#practical-edge-cases-and-easy-fixes","title":"Practical edge cases (and easy fixes)","text":"<ul> <li>(1,1,1) looks \u201ctoo simple.\u201d That\u2019s often fine\u2014short memory is real. If residuals still show structure, try small increments (e.g., \\((2,1,1)\\) or \\((1,1,2)\\)), but accept parsimony when BIC and diagnostics agree.</li> <li>Heteroskedastic residuals but mild. Keep ARIMA for the mean, use robust errors or widen PIs via simulation; only escalate to GARCH if coverage suffers.</li> <li>Drift vs. constant confusion. With \\(d=1\\), use drift (not a level constant). With \\(d=0\\), use a constant if the mean is nonzero.</li> <li>Outliers. Winsorize or add intervention dummies; otherwise a few spikes can spoil inference and intervals.</li> </ul>"},{"location":"theory/arima/#how-autoarimaexplorer-exploits-this-regime","title":"How AutoArimaExplorer exploits this regime","text":"<ul> <li>Two natural working series. Levels standardized for \\(d\\ge 1\\); log-returns for \\(d=0\\).</li> <li>Robust fitting policy. Multiple estimation \u201ctrials\u201d (innovations MLE / state space, with/without constraints) and selection by BIC.</li> <li>Guardrails by design. Residual Ljung\u2013Box (mean, optionally squared), finite-BIC checks, and convergence enforcement.</li> <li>Flexible selection. Best-by-BIC, best-by-\\(q\\) (parsimonious within each MA order), and rolling OOS selection with RMSE/MAE/MAPE or Winkler score for interval calibration.</li> </ul> <p>If your data match the sweet-spot properties above, ARIMA\u2014and by extension AutoArimaExplorer\u2014will likely give you fast, interpretable, and reliable forecasts. For everything else, ARIMA remains a solid baseline and a powerful diagnostic lens to understand what additional structure (seasonality, exogenous signals, volatility, regimes) you may need.</p> <p>Here\u2019s a ready-to-paste chapter.</p>"},{"location":"theory/arima/#limitations-and-common-data-issues","title":"Limitations and common data issues","text":"<p>ARIMA is a powerful baseline, but it has clear failure modes. Knowing them\u2014and how to mitigate them\u2014will save you time and misleading forecasts.</p>"},{"location":"theory/arima/#1-structural-breaks-and-regime-shifts","title":"1) Structural breaks and regime shifts","text":"<p>Symptom. Sudden level/variance changes, policy interventions, product launches, outages. Residual ACF shows leftover structure; parameters drift across time.</p> <p>Why ARIMA struggles. It assumes fixed linear dynamics. Breaks violate stationarity and parameter constancy.</p> <p>What to do.</p> <ul> <li>Detect/mark breaks (change-point tests, domain knowledge) and add intervention dummies.</li> <li>Fit on post-break segments or use rolling re-estimation.</li> <li>Consider regime-switching or time-varying parameter state-space models if shifts are frequent.</li> </ul>"},{"location":"theory/arima/#2-seasonality-and-calendar-effects","title":"2) Seasonality and calendar effects","text":"<p>Symptom. ACF spikes at seasonal lags (e.g., 7, 24, 12), weekday/holiday patterns.</p> <p>Why ARIMA struggles. Nonseasonal ARIMA can\u2019t encode long-period cyclic dependence.</p> <p>What to do.</p> <ul> <li>Use SARIMA (seasonal ARIMA) or deseasonalize via STL/Prophet-style decomposition and model the remainder.</li> <li>Add calendar regressors (dow, holiday flags) with ARIMA errors (ARIMAX).</li> </ul>"},{"location":"theory/arima/#3-heteroskedasticity-time-varying-volatility","title":"3) Heteroskedasticity (time-varying volatility)","text":"<p>Symptom. Residuals pass mean autocorrelation tests but squared residuals are autocorrelated; prediction intervals too narrow in volatile regimes.</p> <p>Why ARIMA struggles. It models the conditional mean, not the variance.</p> <p>What to do.</p> <ul> <li>Variance-stabilizing transforms (log, Box\u2013Cox).</li> <li>Pair with GARCH (ARIMA-GARCH) or use robust/quantile methods for intervals.</li> </ul>"},{"location":"theory/arima/#4-nonlinearity-and-long-memory","title":"4) Nonlinearity and long memory","text":"<p>Symptom. Smooth trends, thresholds, saturations, persistence that decays very slowly (hyperbolic ACF).</p> <p>Why ARIMA struggles. Linear, short-memory structure can\u2019t capture nonlinear response or fractional integration.</p> <p>What to do.</p> <ul> <li>Add nonlinear terms via exogenous regressors or switch to tree/NN models with temporal features.</li> <li>For long memory, consider ARFIMA.</li> </ul>"},{"location":"theory/arima/#5-outliers-and-heavy-tails","title":"5) Outliers and heavy tails","text":"<p>Symptom. Spikes that dominate fit; residuals with fat tails; unstable parameters.</p> <p>Why ARIMA struggles. MLE is sensitive to extremes; a few points can swing coefficients and BIC.</p> <p>What to do.</p> <ul> <li>Winsorize/clean or model interventions explicitly.</li> <li>Use robust estimation or downweight outliers; validate with rolling OOS metrics.</li> </ul>"},{"location":"theory/arima/#6-missing-data-and-irregular-sampling","title":"6) Missing data and irregular sampling","text":"<p>Symptom. Gaps, uneven time steps, duplicated timestamps.</p> <p>Why ARIMA struggles. Standard estimation assumes a regular grid.</p> <p>What to do.</p> <ul> <li>Impute to a regular cadence (carry-forward, interpolation, model-based) or use state-space Kalman filtering that tolerates missingness.</li> <li>Aggregate/disaggregate carefully; avoid mixing frequencies.</li> </ul>"},{"location":"theory/arima/#7-over-under-differencing","title":"7) Over-/under-differencing","text":"<p>Symptom.</p> <ul> <li>Under-differenced: residual ACF shows slow decay; unit-root tests reject stationarity.</li> <li>Over-differenced: negative lag-1 ACF, overdamped dynamics, loss of signal.</li> </ul> <p>What to do.</p> <ul> <li>Use unit-root tests (ADF/PP/KPSS; see Statistical Concepts chapter) and inspect ACF/PACF on differenced series.</li> <li>Prefer the lowest differencing that yields stationarity; consider drift when \\(d=1\\).</li> </ul>"},{"location":"theory/arima/#8-near-nonstationarity-invertibility-unit-root-edges","title":"8) Near nonstationarity / invertibility (unit-root edges)","text":"<p>Symptom. AR or MA roots close to the unit circle; large standard errors; convergence warnings.</p> <p>Why ARIMA struggles. Likelihood surface is flat near boundary; estimates unstable.</p> <p>What to do.</p> <ul> <li>Enforce stationarity/invertibility constraints during fitting (then relax if needed).</li> <li>Favor parsimonious models; re-test on rolling windows.</li> </ul>"},{"location":"theory/arima/#9-identifiability-and-multicollinearity-of-lags","title":"9) Identifiability and multicollinearity of lags","text":"<p>Symptom. Many AR/MA terms but similar fit; inflated standard errors; BIC barely improves.</p> <p>Why ARIMA struggles. Redundant lags create parameter collinearity.</p> <p>What to do.</p> <ul> <li>Keep \\(p,q\\) small; let BIC and OOS validation prune.</li> <li>Inspect ACF/PACF for minimal useful orders.</li> </ul>"},{"location":"theory/arima/#10-exogenous-drivers-omitted","title":"10) Exogenous drivers omitted","text":"<p>Symptom. Forecasts miss systematic effects (price, weather, promotions); residuals correlate with known signals.</p> <p>What to do.</p> <ul> <li>Use ARIMAX with relevant regressors and keep ARMA terms for the residual autocorrelation.</li> </ul>"},{"location":"theory/arima/#11-small-samples","title":"11) Small samples","text":"<p>Symptom. Unstable estimates; diagnostics inconclusive.</p> <p>What to do.</p> <ul> <li>Limit orders (e.g., \\(p,q \\le 2\\)), use BIC over AIC, and consider hierarchical pooling or simpler baselines.</li> </ul>"},{"location":"theory/arima/#12-long-horizon-forecasts","title":"12) Long-horizon forecasts","text":"<p>Symptom. Intervals explode; mean reverts too fast/slow relative to reality.</p> <p>Why ARIMA struggles. Uncertainty accumulates quickly in integrated processes.</p> <p>What to do.</p> <ul> <li>Communicate scenario ranges; refresh with rolling re-fits; blend with structural or causal models for long-term planning.</li> </ul>"},{"location":"theory/arima/#practical-caveats-tooling-diagnostics","title":"Practical caveats (tooling &amp; diagnostics)","text":"<ul> <li>Convergence &amp; warnings. \u201cNon-stationary AR starts\u201d / \u201cNon-invertible MA starts\u201d are common; they usually trigger robust initialization. True non-convergence is a red flag\u2014tighten constraints, reduce orders, or change optimizer settings.</li> <li>Data leakage. Don\u2019t tune on the full series and report naive test error. Use rolling-origin validation.</li> <li>Multiple testing. Large grids inflate false positives. Penalize with BIC, and confirm with OOS metrics and residual diagnostics.</li> <li>Diagnostics to rely on. ACF/PACF, Ljung\u2013Box (mean, and squared residuals for volatility), unit-root tests, parameter stability, and coverage of prediction intervals. Detailed definitions and procedures live in the Statistical Concepts chapter.</li> </ul>"},{"location":"theory/arima/#how-autoarimaexplorer-mitigates-these-issues","title":"How AutoArimaExplorer mitigates these issues","text":"<ul> <li>Guardrails: Convergence enforcement, finite-BIC checks, and Ljung\u2013Box filters (mean \u00b1 optional squared).</li> <li>Parsimony by default: BIC-driven selection, tie-breaks favoring lower \\(p+q\\).</li> <li>Rolling OOS: Optional out-of-sample scoring (RMSE/MAE/MAPE or Winkler for interval calibration).</li> <li>Flexible preprocessing: Work on log-returns for \\(d=0\\) and standardized levels for \\(d\\ge 1\\); easy to add calendar/exogenous features in downstream workflows.</li> </ul>"},{"location":"theory/arima/#practical-identification-heuristics-modeling-playbook","title":"Practical identification heuristics (modeling playbook)","text":"<p>This is a pragmatic, battle-tested checklist you can follow before, during, and after running ARIMA. It blends classic Box\u2013Jenkins rules with modern validation habits. Use it linearly the first time; later you\u2019ll jump around.</p>"},{"location":"theory/arima/#0-sanity-checks-cleaning","title":"0) Sanity checks &amp; cleaning","text":"<ul> <li>Cadence: enforce a regular time step; resample if needed.</li> <li>Missing: impute sensibly (interpolate short gaps, domain-aware fill for long gaps).</li> <li>Outliers: flag large spikes; either explain (events) or winsorize/mark with dummies.</li> <li>Scale/units: if the series is strictly positive and multiplicative, consider log (or Box\u2013Cox).</li> </ul>"},{"location":"theory/arima/#1-first-look","title":"1) First look","text":"<ul> <li>Plot levels and first differences; annotate obvious breaks/seasonality.</li> <li>Inspect a quick ACF/PACF of levels and of first differences.</li> <li>Compute rolling mean/variance to spot heteroskedasticity.</li> </ul>"},{"location":"theory/arima/#2-make-it-stationary-choose-d","title":"2) Make it stationary (choose \\(d\\))","text":"<ul> <li>If levels show slow persistence and ACF decays very slowly \u2192 difference once (\\(d=1\\)).</li> <li>If still non-stationary (trend in mean), test again; rarely \\(d=2\\) is justified.</li> <li>Avoid over-differencing: if you see a strong negative lag-1 ACF in the differenced series, you likely went too far.</li> <li>If \\(d=1\\), consider allowing a drift (constant after differencing) unless domain knowledge says otherwise.</li> </ul> <p>Minimal differencing that achieves stationarity beats aggressive differencing.</p>"},{"location":"theory/arima/#3-seasonal-structure-if-applicable","title":"3) Seasonal structure (if applicable)","text":"<ul> <li>ACF spikes at lag \\(s\\) and multiples (e.g., 7, 12, 24) \u2192 consider seasonal differencing \\(D=1\\) and a SARIMA (not covered in this chapter), or pre-deseasonalize with STL and model the remainder.</li> </ul>"},{"location":"theory/arima/#4-choose-small-candidate-orders-p-q","title":"4) Choose small candidate orders (\\(p, q\\))","text":"<p>Use the differenced (stationary) series\u2019 ACF/PACF as a guide, not a law:</p> <ul> <li>AR(\\(p\\)) hint: PACF cuts off after \\(p\\); ACF tails off.</li> <li>MA(\\(q\\)) hint: ACF cuts off after \\(q\\); PACF tails off.</li> <li>ARMA(\\(p,q\\)) hint: both ACF and PACF tail off.</li> </ul> <p>Start with a small grid: \\(p,q \\in \\{0,1,2\\}\\). Favor parsimony.</p>"},{"location":"theory/arima/#5-fit-compare-in-sample-guardrails","title":"5) Fit &amp; compare (in-sample guardrails)","text":"<p>For each candidate:</p> <ul> <li>Fit with constraints to enforce stationarity/invertibility (relax only if needed).</li> <li> <p>Check:</p> </li> <li> <p>Convergence: must be true (don\u2019t negotiate this).</p> </li> <li>BIC: prefer the smallest; use AIC only for exploratory tie-breaks.</li> <li>Residual diagnostics: Ljung\u2013Box on residuals (no autocorrelation). Optionally test squared residuals to screen for short-range volatility (ARCH-like behavior).</li> </ul> <p>Stopping rules (handy):</p> <ul> <li>Only accept a more complex model if BIC drops by \u2265 2 (or your chosen threshold).</li> <li>If LB fails, discard regardless of BIC.</li> </ul>"},{"location":"theory/arima/#6-cross-validate-out-of-sample-reality-check","title":"6) Cross-validate (out-of-sample reality check)","text":"<ul> <li>Run rolling-origin evaluation with a short horizon (e.g., 1\u20135 steps).</li> <li>Score with RMSE/MAE/MAPE; for interval quality use the Winkler score.</li> <li>Prefer the model that is competitive on BIC and clearly best OOS. If BIC and OOS disagree, trust OOS.</li> </ul>"},{"location":"theory/arima/#7-volatility-intervals","title":"7) Volatility &amp; intervals","text":"<ul> <li>If residuals pass LB but squared residuals fail LB \u2192 variance is time-varying.</li> <li>Expect under-covered prediction intervals; consider variance-stabilizing transforms (log/Box\u2013Cox) or pairing with GARCH for volatility.</li> </ul>"},{"location":"theory/arima/#8-edgeboundary-situations","title":"8) Edge/boundary situations","text":"<ul> <li>Near-unit roots: parameters unstable, big SEs, optimizer warnings. Prefer simpler \\(p,q\\); re-estimate on rolling windows.</li> <li>Identifiability: if adding lags barely improves BIC and parameters are wobbly, prune back.</li> </ul>"},{"location":"theory/arima/#9-checklist-for-good-arima","title":"9) Checklist for \u201cgood\u201d ARIMA","text":"<ul> <li>Converged fit \u2714</li> <li>Smallest (or near-smallest) BIC among parsimonious candidates \u2714</li> <li>Residual Ljung\u2013Box (mean) passes at multiple lags \u2714</li> <li>(Optional) LB on squared residuals passes for short-range lags \u2714</li> <li>Reasonable, stable coefficients (no wild standard errors) \u2714</li> <li>Rolling OOS error competitive or best \u2714</li> <li>Intervals with plausible coverage \u2714</li> </ul>"},{"location":"theory/arima/#a-tiny-acfpacf-cheat-sheet","title":"A tiny ACF/PACF cheat-sheet","text":"<ul> <li>AR(1): PACF spike at lag 1 then \\~0; ACF decays geometrically.</li> <li>MA(1): ACF spike at lag 1 then \\~0; PACF decays geometrically.</li> <li>ARMA(1,1): Both ACF and PACF decay (no sharp cut-off).</li> <li>Over-differenced: strong negative ACF at lag 1.</li> </ul>"},{"location":"theory/arima/#what-this-looks-like-in-practice-with-autoarimaexplorer","title":"What this looks like in practice with AutoArimaExplorer","text":"<ul> <li>Use <code>fit_grid</code> on a small, sensible grid; force <code>require_converged=True</code>.</li> <li>Select with <code>select_best_by_bic</code> (enable LB guardrail; optionally <code>check_sq=True</code>).</li> <li>Cross-check with <code>select_by_oos</code> (rolling origin) on the finalists.</li> <li>If LB on squared residuals fails repeatedly, consider a transform or volatility model.</li> <li>Keep models simple unless there is clear BIC and OOS justification.</li> </ul> <p>Rule of thumb: if a model is only microscopically better in-sample but worse OOS or fails diagnostics, it\u2019s not better.</p>"},{"location":"theory/arima/#extensions-you-should-know","title":"Extensions you should know","text":"<ul> <li>SARIMA \\((p,d,q)\\times(P,D,Q)_s\\): seasonal ARIMA with seasonal difference \\(D\\) and seasonal period \\(s\\).</li> <li>ARIMAX / SARIMAX: add exogenous regressors \\(X_t\\) to capture calendar, promotions, weather, etc.</li> <li>State-space / Kalman: ARIMA recast; enables missing data handling, time-varying parameters, and smoother-based diagnostics.</li> <li>ARFIMA: fractional differencing to model long memory.</li> <li>Regime-switching ARIMA: different parameter sets by latent regime.</li> </ul>"},{"location":"theory/arima/#troubleshooting-checklist","title":"Troubleshooting checklist","text":"<ul> <li>Residual ACF significant at short lags? Increase \\(p\\) or \\(q\\) (not both at once), or reconsider differencing.</li> <li>Large negative MA estimates near \u22121 with \\(d=1\\)? Potential over-differencing.</li> <li>Non-invertible MA or non-stationary AR warnings? Re-specify orders; enforce constraints during estimation.</li> <li>Prediction intervals too narrow/wide? Suspect heteroskedasticity; consider GARCH or robust intervals.</li> <li>Seasonal spikes in ACF/PACF? Add seasonal difference \\(D=1\\) and seasonal AR/MA terms.</li> <li>Sudden fit degradation? Look for breaks; add interventions or refit on a rolling window.</li> </ul>"},{"location":"theory/arima/#how-this-theory-guides-autoarimaexplorer","title":"How this theory guides AutoArimaExplorer","text":"<ul> <li>Uses standardized levels for \\(d\\ge 1\\) and log-returns for \\(d=0\\) to respect stationarity assumptions.</li> <li>Attempts multiple estimation trials (trend and constraint variants) and only keeps converged, finite-BIC fits.</li> <li>Provides guardrails (Ljung\u2013Box on residuals and optionally on squared residuals).</li> <li>Offers three selection lenses: overall Best-by-BIC, per-\\((d,q)\\) Best-by-q (parsimonious stepwise path), and Best-by-OOS via rolling validation (RMSE/MAE/MAPE or Winkler for interval calibration).</li> </ul> <p>Use ARIMA when you need interpretable, fast, and competitive short-horizon forecasts\u2014and lean on diagnostics and OOS checks to keep yourself honest.</p>"},{"location":"theory/arima/#where-the-stats-live-and-what-to-read-next","title":"Where the stats live (and what to read next)","text":"<p>This chapter focuses on what ARIMA is and when it works. The statistical machinery we rely on\u2014tests, estimators, diagnostics, and selection criteria\u2014is treated in depth in the companion chapter Statistical Concepts. If you want the math behind each knob we turn in AutoArimaExplorer, read that next. Specifically, it covers:</p> <ul> <li>Stationarity &amp; unit roots: ADF, KPSS, and the practical meaning of \\(d\\).</li> <li>Correlation structure: ACF/PACF definitions, sample estimators, and sampling variability.</li> <li>Model adequacy tests: Ljung\u2013Box on residuals and on squared residuals (ARCH check).</li> <li>Information criteria: AIC, AICc, BIC\u2014derivations, penalties, and when each is preferable.</li> <li>Estimation details: MLE for ARIMA, state-space likelihood, constraints (stationarity/invertibility).</li> <li>Forecast uncertainty: prediction vs. confidence intervals; what assumptions underpin them.</li> <li>Error metrics: RMSE/MAE/MAPE, coverage and Winkler score for prediction intervals.</li> <li>Transformations &amp; scaling: log/Box\u2013Cox, standardization, and their impact on inference.</li> <li>Model selection trade-offs: parsimony, overfitting, bias\u2013variance, and tie-breaking rules.</li> </ul> <p>Use this ARIMA chapter for the modeling storyline; use Statistical Concepts when you want the proofs, formulas, and the fine print behind each diagnostic and selection choice.</p>"},{"location":"theory/introduction/","title":"Introduction","text":"<p>Forecasting with time series is, at its core, a game of exploiting memory. Most real-world signals\u2014sales, loads, prices, churn\u2014do not reset to zero at midnight; what happened yesterday leaks into what happens today. ARIMA-class models are the classical way to formalize that memory: compact, interpretable and (when used where they fit) remarkably hard to beat for short-horizon forecasting.</p> <p>This chapter gives you the conceptual toolkit to read, question, and effectively use our AutoArimaExplorer. I\u2019ll keep the math light here and focus on intuition; the statistical details and proofs live in the Statistical Concepts chapter.</p>"},{"location":"theory/introduction/#a-mental-model-for-time-series","title":"A mental model for time series","text":"<p>Think of a univariate time series \\(x_t\\) as \u201cstructure + noise\u201d observed at regular intervals. The structure we care about is serial dependence\u2014today\u2019s value being explained by its own past and by past \u201cshocks.\u201d The noise, or innovation \\(\\varepsilon_t\\), is what\u2019s left once we\u2019ve explained everything systematic.</p> <p>Two complementary mechanisms capture this dependence:</p> <ul> <li>Autoregression (AR): feedback from the past level. If \\(x_t\\) tends to move back toward a long-run mean after a shock, or if it carries inertia forward, that\u2019s AR behavior.</li> <li>Moving average (MA): echoes of past shocks. A one-off shock rarely vanishes instantly; it ripples for a few periods. MA terms model those ripples.</li> </ul> <p>An ARMA(p,q) model combines both: the current value depends on the last \\(p\\) levels and the last \\(q\\) shock echoes. This is a parsimonious way to capture short-run dynamics without overfitting.</p>"},{"location":"theory/introduction/#from-arma-to-arima","title":"From ARMA to ARIMA","text":"<p>ARMA assumes the series is stationary in mean and variance\u2014no drifting level, no deterministic trend. Many business and economic series aren\u2019t. The standard cure is differencing: model the changes rather than the levels.</p> <p>An ARIMA(p,d,q) model simply applies \\(d\\) differences to the original series and then fits an ARMA on that differenced series. Intuitively:</p> <ul> <li>\\(d=0\\): the series is already \u201cstable\u201d \u2192 ARMA is fine.</li> <li>\\(d=1\\): first differences (like returns) are stable \u2192 ARMA on \\(\\Delta x_t\\).</li> <li>\\(d\\ge 2\\): more aggressive detrending if changes themselves trend.</li> </ul> <p>Differencing restores stationarity in mean, letting AR/MA terms describe the short-run correlation structure cleanly. The art is to difference enough to kill the drift but not so much that you destroy meaningful signal (over-differencing can inject spurious MA structure and inflate forecast variance).</p>"},{"location":"theory/introduction/#where-arima-shines-and-where-it-doesnt","title":"Where ARIMA shines (and where it doesn\u2019t)","text":"<p>Strengths</p> <ul> <li>Short-horizon accuracy: Excellent for 1\u201310 step forecasts when dynamics are primarily short-memory.</li> <li>Interpretability: Coefficients map to persistence and shock decay; diagnostics map to \u201cis anything left in the residuals?\u201d</li> <li>Parsimony: Small number of parameters; robust on modest sample sizes.</li> <li>Speed &amp; stability: Mature estimation routines with well-understood behavior.</li> </ul> <p>Limitations &amp; common data issues</p> <ul> <li>Strong seasonality: Plain ARIMA does not encode seasonal cycles; you\u2019d need SARIMA (seasonal ARIMA) or exogenous regressors.</li> <li>Time-varying volatility: Volatility clustering (ARCH/GARCH effects) violates constant-variance assumptions. ARIMA can forecast the mean well while mis-calibrating uncertainty.</li> <li>Structural breaks and regime changes: Coefficients are fixed; abrupt changes can degrade fit.</li> <li>Missing data / irregular sampling: Classical estimators expect regular, clean sampling; pre-processing matters.</li> <li>Outliers: A few large shocks can bias estimation and diagnostics; robust cleaning helps.</li> <li>Long memory: Persistent autocorrelation across long lags may require specialized models.</li> </ul> <p>These are not deal breakers; they\u2019re design constraints. They inform how we prepare data, how strict our diagnostics should be, and when to escalate beyond plain ARIMA.</p>"},{"location":"theory/introduction/#residuals-diagnostics-and-model-adequacy-the-high-level-view","title":"Residuals, diagnostics, and model adequacy (the high-level view)","text":"<p>A good ARIMA fit makes residuals behave like white noise: no leftover autocorrelation in the mean, and\u2014if you also want well-behaved uncertainty\u2014no short-run dependence in squared residuals either. Two practical signals:</p> <ul> <li>Ljung\u2013Box tests on residuals: detect leftover autocorrelation (missed AR/MA structure).</li> <li>Ljung\u2013Box on squared residuals: quick check for ARCH-type effects (time-varying variance).</li> </ul> <p>Model selection balances fit versus complexity. Information criteria\u2014most commonly BIC\u2014penalize extra parameters; out-of-sample rolling tests verify that gains are real, not artifacts. We detail these tools in the Statistical Concepts chapter; here, it\u2019s enough to remember: lower BIC and cleaner residuals are better, but always validate on held-out data when the stakes are high.</p>"},{"location":"theory/introduction/#how-autoarimaexplorer-uses-these-ideas","title":"How AutoArimaExplorer uses these ideas","text":"<p>AutoArimaExplorer operationalizes the theory above into a practical workflow:</p> <ul> <li>Two views of the series: it uses standardized levels for \\(d\\ge 1\\) and log-returns for \\(d=0\\). This mirrors how ARIMA is intended to be used and stabilizes estimation.</li> <li>Robust fitting: multiple estimation \u201ctrials\u201d (changes in trend specification and optimizer constraints) are attempted; only finite-BIC solutions that converge are considered viable.</li> <li>Guardrails by design: optional Ljung\u2013Box checks on residuals (and squared residuals) act as sanity filters.</li> <li> <p>Three complementary selectors:</p> </li> <li> <p>Best-by-BIC across the full grid.</p> </li> <li>Best-by-q (choose \\(p\\) within each \\((d,q)\\) slice) for a stepwise, parsimonious path.</li> <li>Best-by-OOS via rolling-origin evaluation (RMSE/MAE/MAPE or Winkler score for PI calibration).</li> </ul> <p>The result is a model that is interpretable, statistically sound by default, and validated against the kinds of data problems you actually meet in practice.</p>"},{"location":"theory/introduction/#what-youll-take-away","title":"What you\u2019ll take away","text":"<p>After this chapter you should be able to:</p> <ul> <li>Explain, in plain language, what AR, MA and ARIMA components do and when they\u2019re appropriate.</li> <li>Recognize the assumptions ARIMA makes (and how differencing helps meet them).</li> <li>Identify when ARIMA is the right tool\u2014and when to reach for seasonal terms, exogenous regressors, or volatility models.</li> <li>Read AutoArimaExplorer\u2019s selections and diagnostics with a critical eye, knowing what each signal means.</li> </ul> <p>If you want the equations, proofs, and test definitions, jump to Statistical Concepts. If you want to see how these ideas become code and APIs, head to the Technical Concepts section.</p>"},{"location":"theory/statistical_conceps/","title":"Statistical Concepts &amp; Diagnostics","text":"<p>This chapter unpacks the statistical machinery behind ARIMA modeling and behind the guardrails that AutoArimaExplorer uses. It\u2019s deliberately rigorous: formulas, assumptions, and the \u201cwhy\u201d behind each diagnostic and selection rule. If you digest this chapter, you\u2019ll understand every knob we turn.</p>"},{"location":"theory/statistical_conceps/#1-stationarity-unit-roots","title":"1) Stationarity &amp; Unit Roots","text":""},{"location":"theory/statistical_conceps/#11-weak-stationarity","title":"1.1 Weak stationarity","text":"<p>A process \\(\\{x_t\\}\\) is (weakly) stationary if its first two moments do not depend on \\(t\\):</p> <ul> <li>\\(\\mathbb{E}[x_t] = \\mu\\) (constant)</li> <li>\\(\\mathrm{Var}(x_t) = \\gamma(0)\\) (finite, constant)</li> <li>\\(\\mathrm{Cov}(x_t, x_{t+h}) = \\gamma(h)\\) (depends only on lag \\(h\\))</li> </ul>"},{"location":"theory/statistical_conceps/#12-unit-roots-differencing","title":"1.2 Unit roots &amp; differencing","text":"<p>AR polynomial \\(\\phi(B) = 1 - \\phi_1 B - \\dots - \\phi_p B^p\\). A unit root occurs if \\(\\phi(1)=0\\) (i.e., root on the unit circle), implying a stochastic trend. Differencing removes it:</p> <ul> <li>If one unit root \u2192 use first difference \\(\\nabla x_t = x_t - x_{t-1}\\) (i.e., \\(d=1\\)).</li> <li>If two unit roots (rare) \u2192 \\(d=2\\).</li> </ul>"},{"location":"theory/statistical_conceps/#13-adf-and-kpss-tests-complements","title":"1.3 ADF and KPSS tests (complements)","text":"<p>Augmented Dickey\u2013Fuller (ADF): tests \\(H_0\\): unit root (non-stationary) vs \\(H_1\\): stationary. Regression (one version):</p> \\[ \\Delta x_t = \\alpha + \\beta t + \\rho x_{t-1} + \\sum_{i=1}^k \\psi_i \\Delta x_{t-i} + u_t. \\] <p>Rejecting \\(H_0\\) (small p-value) supports stationarity.</p> <p>KPSS: tests \\(H_0\\): level/trend stationarity vs \\(H_1\\): unit root. Statistic:</p> \\[ \\mathrm{KPSS} = \\frac{1}{n^2 \\hat{\\sigma}_u^2} \\sum_{t=1}^{n} S_t^2,\\quad S_t = \\sum_{i=1}^{t} \\hat{u}_i, \\] <p>with \\(\\hat{\\sigma}_u^2\\) a long-run variance estimator. Large values \u2192 reject stationarity.</p> <p>Practical rule: Use ADF and KPSS together:</p> <ul> <li>ADF reject &amp; KPSS not reject \u2192 stationary.</li> <li>ADF not reject &amp; KPSS reject \u2192 non-stationary, difference.</li> <li>Mixed/inconclusive \u2192 inspect plots, ACF, and domain context.</li> </ul>"},{"location":"theory/statistical_conceps/#2-correlation-structure-acf-pacf","title":"2) Correlation Structure: ACF &amp; PACF","text":""},{"location":"theory/statistical_conceps/#21-definitions","title":"2.1 Definitions","text":"<p>Autocovariance: \\(\\gamma(h) = \\mathrm{Cov}(x_t, x_{t+h})\\). Autocorrelation: \\(\\rho(h) = \\gamma(h)/\\gamma(0)\\).</p> <p>Sample estimators for series \\(\\{x_t\\}_{t=1}^n\\) with mean \\(\\bar{x}\\):</p> \\[ \\hat{\\gamma}(h) = \\frac{1}{n}\\sum_{t=1}^{n-h} (x_t - \\bar{x})(x_{t+h} - \\bar{x}),\\quad \\hat{\\rho}(h) = \\frac{\\hat{\\gamma}(h)}{\\hat{\\gamma}(0)}. \\] <p>Partial ACF (PACF) at lag \\(k\\) is the correlation between \\(x_t\\) and \\(x_{t-k}\\) after linearly removing the effect of intermediate lags \\(1,\\dots,k-1\\). Computationally via Durbin\u2013Levinson / Yule\u2013Walker recursions.</p>"},{"location":"theory/statistical_conceps/#22-sampling-variability-bands","title":"2.2 Sampling variability &amp; bands","text":"<p>Under white-noise null, approximate 95% bands:</p> \\[ \\hat{\\rho}(h) \\approx \\mathcal{N}\\!\\left(0, \\frac{1}{n}\\right) \\;\\Rightarrow\\; \\text{bands} \\approx \\pm \\frac{1.96}{\\sqrt{n}}. \\] <p>Use with care: data-driven selection inflates false positives; treat ACF/PACF as guides, not proofs.</p>"},{"location":"theory/statistical_conceps/#3-model-adequacy-ljungbox","title":"3) Model Adequacy: Ljung\u2013Box","text":"<p>The Ljung\u2013Box statistic aggregates sample autocorrelations up to lag \\(m\\):</p> \\[ Q(m) = n(n+2)\\sum_{h=1}^{m} \\frac{\\hat{\\rho}(h)^2}{n-h}. \\] <p>Under \\(H_0\\) (no autocorrelation), \\(Q(m)\\) is approximately \\(\\chi^2\\) with degrees of freedom \\(m - q\\) if \\(q\\) MA parameters were estimated (practical df adjustments vary).</p> <ul> <li>On residuals: Checks leftover autocorrelation \u2192 misspecification if rejected.</li> <li>On squared residuals: Checks short-range conditional heteroskedasticity (ARCH-like effects).</li> </ul> <p>AutoArimaExplorer guardrail: A candidate must pass LB on residuals (and optionally on residuals\u00b2) at several lags to be considered adequate.</p>"},{"location":"theory/statistical_conceps/#4-information-criteria-aic-aicc-bic","title":"4) Information Criteria: AIC, AICc, BIC","text":"<p>Let \\(\\hat{\\ell}\\) be the maximized log-likelihood and \\(k\\) the number of estimated parameters (including variance).</p> <ul> <li>AIC: \\(\\mathrm{AIC} = -2\\hat{\\ell} + 2k\\)</li> <li>AICc (small-sample correction):</li> </ul> \\[ \\mathrm{AICc} = \\mathrm{AIC} + \\frac{2k(k+1)}{n-k-1} \\] <ul> <li>BIC: \\(\\mathrm{BIC} = -2\\hat{\\ell} + k\\log n\\)</li> </ul> <p>Guidance:</p> <ul> <li>AIC/AICc often favor better predictive models; AICc when \\(n/k\\) is small.</li> <li>BIC is more parsimonious; consistent for true model order under ideal conditions.</li> <li>We typically use BIC to control complexity and prevent overfitting, with a drop threshold (e.g., \\(\\Delta \\mathrm{BIC} \\ge 2\\)) to justify extra parameters.</li> </ul>"},{"location":"theory/statistical_conceps/#5-estimation-mle-state-space-likelihood","title":"5) Estimation: MLE &amp; State-Space Likelihood","text":""},{"location":"theory/statistical_conceps/#51-arima-likelihood-invertible-representation","title":"5.1 ARIMA likelihood (invertible representation)","text":"<p>ARIMA(\\(p,d,q\\)) can be written for the differenced series as an ARMA(\\(p,q\\)) with i.i.d. innovations:</p> \\[ \\phi(B)\\, y_t = \\theta(B)\\, \\varepsilon_t,\\quad \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2), \\] <p>where \\(y_t = \\nabla^d x_t\\). Under Gaussianity, the log-likelihood follows from the joint normal density of residuals.</p>"},{"location":"theory/statistical_conceps/#52-state-space-kalman-filter","title":"5.2 State-space &amp; Kalman filter","text":"<p>ARIMA admits a linear Gaussian state-space form:</p> \\[ \\begin{aligned} \\text{State: } &amp; \\alpha_{t+1} = T \\alpha_t + R \\eta_t, \\\\ \\text{Obs: }   &amp; y_t = Z \\alpha_t + \\varepsilon_t, \\end{aligned} \\] <p>with \\(\\eta_t,\\varepsilon_t\\) Gaussian. The Kalman filter/smoother yields the exact likelihood (up to initialization choices). This is robust and handles missing data naturally.</p>"},{"location":"theory/statistical_conceps/#53-constraints","title":"5.3 Constraints","text":"<ul> <li>Stationarity: all roots of \\(\\phi(z)=0\\) outside unit circle.</li> <li>Invertibility: all roots of \\(\\theta(z)=0\\) outside unit circle.</li> </ul> <p>Implementations enforce this via constrained optimization or parameter transforms (e.g., hyperbolic tangent reparameterization). AutoArimaExplorer lets you choose strict enforcement or relaxed attempts when the optimizer struggles.</p>"},{"location":"theory/statistical_conceps/#6-forecasts-uncertainty","title":"6) Forecasts &amp; Uncertainty","text":""},{"location":"theory/statistical_conceps/#61-point-forecasts","title":"6.1 Point forecasts","text":"<p>For ARIMA(\\(p,d,q\\)), \\(h\\)-step forecasts are linear projections based on the ARMA representation of \\(y_t=\\nabla^d x_t\\), then integrated back to the \\(x_t\\) scale.</p>"},{"location":"theory/statistical_conceps/#62-forecast-variance-intervals","title":"6.2 Forecast variance &amp; intervals","text":"<p>For an ARMA(\\(p,q\\)) with innovation variance \\(\\sigma^2\\), the \\(h\\)-step forecast error variance is</p> \\[ \\mathrm{Var}(\\hat{y}_{t+h|t} - y_{t+h}) = \\sigma^2 \\sum_{j=0}^{h-1} \\psi_j^2, \\] <p>where \\(\\{\\psi_j\\}\\) are the MA(\\(\\infty\\)) coefficients (impulse response). Intervals (Gaussian):</p> \\[ \\hat{y}_{t+h|t} \\pm z_{1-\\alpha/2}\\,\\sigma_h. \\] <p>After integrating to the \\(x\\)-scale, intervals widen appropriately.</p>"},{"location":"theory/statistical_conceps/#63-back-transform-bias","title":"6.3 Back-transform bias","text":"<p>If you model \\(\\log x_t\\), then</p> \\[ \\mathbb{E}[x_{t+h}| \\mathcal{F}_t] \\approx \\exp\\!\\big(\\mu_h + \\tfrac{1}{2}\\sigma_h^2\\big), \\] <p>not \\(\\exp(\\mu_h)\\). Use bias-correction when reporting mean forecasts on the original scale.</p>"},{"location":"theory/statistical_conceps/#7-error-metrics-interval-quality","title":"7) Error Metrics &amp; Interval Quality","text":"<p>Let \\(y_i\\) be actuals and \\(\\hat{y}_i\\) forecasts.</p> <ul> <li>RMSE: \\(\\sqrt{\\frac{1}{N}\\sum_i (y_i - \\hat{y}_i)^2}\\) (penalizes large errors)</li> <li>MAE: \\(\\frac{1}{N}\\sum_i |y_i - \\hat{y}_i|\\) (robust to outliers)</li> <li>MAPE: \\(\\frac{100}{N}\\sum_i \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|\\) (undefined at zeros; be careful)</li> </ul> <p>Prediction interval quality: For a nominal \\((1-\\alpha)\\) interval \\([L_i, U_i]\\), the Winkler score:</p> \\[ W_\\alpha(y_i, L_i, U_i) = \\begin{cases} U_i - L_i, &amp; L_i \\le y_i \\le U_i,\\\\ (U_i - L_i) + \\frac{2}{\\alpha}(L_i - y_i), &amp; y_i &lt; L_i,\\\\ (U_i - L_i) + \\frac{2}{\\alpha}(y_i - U_i), &amp; y_i &gt; U_i. \\end{cases} \\] <p>Lower is better; it balances narrowness and coverage. AutoArimaExplorer supports Winkler-based OOS selection.</p>"},{"location":"theory/statistical_conceps/#8-transformations-scaling","title":"8) Transformations &amp; Scaling","text":"<ul> <li>Log / Box\u2013Cox: stabilize variance, linearize growth. Box\u2013Cox:</li> </ul> \\[ x^{(\\lambda)} = \\begin{cases} \\dfrac{x^\\lambda - 1}{\\lambda}, &amp; \\lambda \\ne 0,\\\\[6pt] \\log x, &amp; \\lambda=0. \\end{cases} \\] <ul> <li>Standardization: \\((x - \\mu)/\\sigma\\) improves optimizer stability; predictions are back-scaled.</li> <li>Min\u2013max [0,1]: helpful for comparability in OOS scoring across different \\(d\\) choices (used optionally).</li> </ul> <p>Note: AIC/BIC are invariant to scale changes that reparameterize \\(\\sigma^2\\) appropriately, but diagnostics like MAPE are not; be consistent when comparing models.</p>"},{"location":"theory/statistical_conceps/#9-model-selection-trade-offs","title":"9) Model Selection Trade-offs","text":"<ul> <li>Parsimony vs fit: More parameters always improve in-sample likelihood; information criteria penalize this.</li> <li>Bias\u2013variance: Simple models may be biased but generalize better; complex models risk variance/instability.</li> <li>Tie-breaking: If BIC is effectively tied, prefer the model with smaller \\(p+q\\); then smaller \\(p\\), then \\(q\\). AutoArimaExplorer uses this hierarchy.</li> <li>Out-of-sample supremacy: When diagnostics and BIC disagree with OOS performance, trust OOS (with enough folds).</li> </ul>"},{"location":"theory/statistical_conceps/#10-volatility-misspecification-breaks","title":"10) Volatility, Misspecification &amp; Breaks","text":"<ul> <li>Conditional heteroskedasticity (ARCH/GARCH): If LB on squared residuals rejects, ARIMA\u2019s intervals will be miscalibrated. Consider variance-stabilizing transforms, or model volatility explicitly (e.g., ARIMA + GARCH on residuals).</li> <li>Structural breaks / regime changes: ARIMA assumes time-invariant parameters. Breaks induce spurious autocorrelation and poor forecasts. Detect via change-point tests or rolling fits.</li> <li>Seasonality: Strong seasonal autocorrelation calls for seasonal differencing and SARIMA (or pre-decomposition like STL).</li> <li>Exogenous effects (ARIMAX): Shocks, promotions, holidays, weather, etc., create predictable structure not captured by pure ARIMA. Consider regressors.</li> </ul>"},{"location":"theory/statistical_conceps/#11-how-autoarimaexplorer-uses-these-concepts","title":"11) How AutoArimaExplorer uses these concepts","text":"<ul> <li>Stationarity handling: Tries \\(d=0,1,\\dots\\) (you control the grid). If \\(d=0\\), works on log-returns; if \\(d\\ge1\\), works on standardized levels.</li> <li>Estimation: Robust MLE via state-space Kalman likelihood (and innovations MLE for \\(d=0\\)), with retries under different trend/constraint setups.</li> <li>Diagnostics: Mandatory convergence; Ljung\u2013Box guardrail on residuals (and optional on squared residuals).</li> <li> <p>Selection:</p> </li> <li> <p>In-sample: BIC minimization with a drop threshold (e.g., \\(\\Delta \\mathrm{BIC}\\ge2\\)).</p> </li> <li>By-q: best \\(p\\) per \\((d,q)\\), then compare across \\((d,q)\\).</li> <li>Out-of-sample (rolling origin): RMSE/MAE/MAPE or Winkler for intervals; the OOS winner can override purely in-sample picks.</li> </ul>"},{"location":"theory/statistical_conceps/#12-references-suggested","title":"12) References (suggested)","text":"<ul> <li>Box, Jenkins, Reinsel &amp; Ljung (2015): Time Series Analysis: Forecasting and Control.</li> <li>Brockwell &amp; Davis (2016): Introduction to Time Series and Forecasting.</li> <li>Hyndman &amp; Athanasopoulos (2021): Forecasting: Principles and Practice.</li> <li>Hamilton (1994): Time Series Analysis.</li> <li>Ljung &amp; Box (1978): On a Measure of Lack of Fit in Time Series Models.</li> <li>Akaike (1974); Schwarz (1978): Original AIC/BIC papers.</li> </ul>"}]}