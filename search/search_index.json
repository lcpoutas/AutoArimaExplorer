{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"arima/","title":"Arima","text":"<p>love it\u2014totally agree. Here\u2019s a tightened chapter draft (English) that first gives context on time series and ARMA, then introduces ARIMA, and focuses on advantages, limitations, and common data issues. I\u2019ve kept model-selection/descriptive-statistics out (that will live in your \u201cStatistical Concepts\u201d section).</p>"},{"location":"arima/#time-series-arma-and-arima-concepts-practicalities","title":"Time Series, ARMA, and ARIMA \u2014 Concepts &amp; Practicalities","text":""},{"location":"arima/#0-what-this-chapter-is-and-isnt","title":"0) What this chapter is (and isn\u2019t)","text":"<p>This chapter gives the conceptual grounding for ARIMA modeling: what time series are, what ARMA models capture, how ARIMA extends ARMA, and the strengths/limitations you should expect in practice. We do not cover detailed estimation, model selection rules, diagnostic tests, or descriptive statistics here\u2014those belong to the \u201cStatistical Concepts\u201d chapter.</p>"},{"location":"arima/#1-time-series-in-a-nutshell","title":"1) Time series in a nutshell","text":"<p>A time series is an ordered sequence ${y_t}$, usually dependent over time. Two recurring ideas:</p> <ul> <li>Dependence in time: yesterday informs today; patterns often decay with lag.</li> <li>Stability vs. change: many workflows assume some form of stationarity\u2014that the series\u2019 statistical character doesn\u2019t drift too much over time. Real data often violate this (trends, breaks, seasonality).</li> </ul> <p>Common real-world nuisances: missing values, outliers/jumps, calendar effects, regime changes, seasonality, and changing variance (heteroskedasticity).</p>"},{"location":"arima/#2-arma-short-run-structure-of-a-stationary-series","title":"2) ARMA: short-run structure of a stationary series","text":"<p>An ARMA(p, q) describes a stationary series $x_t$ via:</p> <ul> <li>AR(p) (AutoRegressive): dependence on past values $x_{t-1},\\dots,x_{t-p}$.</li> <li>MA(q) (Moving Average): dependence on past one-step forecast errors (innovations).</li> </ul> <p>Intuition:</p> <ul> <li>AR terms encode gradual persistence/mean reversion in the mean.</li> <li>MA terms encode serially correlated shocks (e.g., short blips/ripples).</li> </ul> <p>ARMA assumes:</p> <ul> <li>Linear dynamics in the mean.</li> <li>Constant innovation variance (no explicit volatility dynamics).</li> <li>No strong seasonality unless explicitly modeled.</li> </ul>"},{"location":"arima/#3-from-arma-to-arima-removing-low-frequency-structure","title":"3) From ARMA to ARIMA: removing low-frequency structure","text":"<p>Many series aren\u2019t stationary in level. ARIMA(p, d, q) first differences the original series $y_t$ $d$ times to remove low-frequency components (e.g., trends), producing $x_t = (1 - B)^d y_t$. Then it fits an ARMA(p, q) to $x_t$.</p> <ul> <li>$d=0$: plain ARMA on a stationary target (e.g., log-returns).</li> <li>$d=1$: first differences remove linear trends (common when modeling levels).</li> <li>$d \\ge 2$: rare in practice; each differencing step removes lower-order polynomial trends but raises noise and can overfit short-run MA structure.</li> </ul> <p>Drift/trend note. After differencing, adding a constant to $x_t$ implies a polynomial trend in $y_t$ (e.g., drift with $d=1$ \u2248 linear trend in level series).</p>"},{"location":"arima/#4-when-arima-is-a-good-idea","title":"4) When ARIMA is a good idea","text":"<ul> <li>You want a transparent, lightweight model of short-run linear dynamics.</li> <li>The target is (or can be made) roughly stationary after modest transformation/differencing.</li> <li>Forecast horizons are short to medium, where mean dynamics matter and seasonal/volatility effects are limited or separately handled.</li> </ul> <p>When modeling financial data:</p> <ul> <li>Log-returns often suit $d=0$ ARMA (stationary mean, simpler dynamics).</li> <li>Prices/levels may need $d=1$, but forecasts often mirror cumulative return behavior with added uncertainty.</li> </ul>"},{"location":"arima/#5-advantages","title":"5) Advantages","text":"<ul> <li>Interpretability: AR and MA terms correspond to recognizable patterns in serial dependence.</li> <li>Parsimony: Small $p+q$ often suffices for short-run structure.</li> <li>Speed &amp; simplicity: Fast to fit; easy to monitor and automate.</li> <li>Strong baseline: A robust control model to beat with more complex approaches.</li> </ul>"},{"location":"arima/#6-limitations-know-these-upfront","title":"6) Limitations (know these upfront)","text":"<ul> <li>Mean only. ARIMA models the conditional mean; it ignores time-varying volatility, fat tails, and asymmetries.</li> <li>Linearity. Nonlinear dynamics, thresholds, and regime switches aren\u2019t captured.</li> <li>Sensitivity to differencing. Over- or under-differencing can distort dynamics, inflate variance, or leave residual structure.</li> <li>Weak identifiability in richer orders. AR and MA components can (nearly) cancel, making parameters unstable and optimization finicky.</li> <li>No built-in seasonality or exogenous drivers. You need SARIMA for seasonality and ARIMAX for regressors.</li> </ul>"},{"location":"arima/#7-common-data-problems-and-why-they-matter","title":"7) Common data problems (and why they matter)","text":"<ul> <li>Missing values / ragged edges: Gaps disturb dependence patterns; careful imputation or model-based handling is needed.</li> <li>Outliers and jumps: Single shocks can mimic MA terms; ignoring them can bias order selection and degrade forecasts.</li> <li>Structural breaks / regime shifts: One model for all periods can be misleading; consider segmentation or time-varying models.</li> <li>Seasonality / calendar effects: Daily/weekly/holiday components can dominate; use SARIMA or include regressors.</li> <li>Heteroskedasticity (volatility clustering): Common in finance; ARIMA residuals may look \u201cwhite\u201d in mean but not in variance\u2014consider ARIMA + GARCH or related volatility models.</li> <li>Nonstationary variance: Transformations (log, Box\u2013Cox) may stabilize amplitude before differencing.</li> </ul>"},{"location":"arima/#8-typical-modeling-pitfalls","title":"8) Typical modeling pitfalls","text":"<ul> <li>Under-differencing: persistent low-lag autocorrelation remains; forecasts drift.</li> <li>Over-differencing: noisy series with spurious MA behavior; wider, less reliable intervals.</li> <li>Too many parameters: high $p+q$ fits noise, yields unstable estimates and fragile forecasts.</li> <li>Roots near unity: near-unit AR/MA roots create very persistent dynamics and optimization warnings; predictions become highly uncertain.</li> <li>Confusing residual diagnostics: Passing \u201cwhiteness\u201d in mean does not guarantee calibrated intervals or stable OOS performance, especially with heteroskedasticity.</li> </ul>"},{"location":"arima/#9-extensions-and-alternatives-when-arima-isnt-enough","title":"9) Extensions and alternatives (when ARIMA isn\u2019t enough)","text":"<ul> <li>SARIMA (seasonal ARIMA): seasonal AR/MA and seasonal differencing for periodic patterns.</li> <li>ARIMAX / Dynamic Regression: ARIMA for residual dependence + exogenous covariates.</li> <li>ARIMA + GARCH (or other volatility models): mean + time-varying variance for financial series.</li> <li>State-space / Kalman filter models: time-varying parameters, local trends, structural components.</li> <li>Exponential smoothing / ETS: trend/seasonal decomposition with a different philosophy (often competitive for forecasting).</li> <li>Machine learning / gradient boosting / tree ensembles: if strong nonlinearities or rich covariates dominate.</li> <li>Neural sequence models: when long and complex nonlinear dependencies matter and data are abundant.</li> </ul>"},{"location":"arima/#10-forecasts-and-uncertainty-high-level-view","title":"10) Forecasts and uncertainty (high-level view)","text":"<p>ARIMA yields:</p> <ul> <li>Point forecasts from the linear recursion in the mean.</li> <li>Uncertainty bands from innovation variance and the MA representation.   In heteroskedastic or heavy-tailed settings, nominal intervals can be too narrow; empirical checks via rolling validation or variance models improve calibration.</li> </ul>"},{"location":"arima/#11-practical-when-to-use-when-to-not","title":"11) Practical \u201cwhen to use / when to not\u201d","text":"<p>Use ARIMA when:</p> <ul> <li>You need a fast, interpretable baseline.</li> <li>Short-run linear dependence is present after mild transformation/differencing.</li> <li>You are building a pipeline that must be robust and auditable.</li> </ul> <p>Avoid or extend ARIMA when:</p> <ul> <li>Seasonality, exogenous drivers, or regime changes dominate.</li> <li>Volatility dynamics are central (finance).</li> <li>Long-horizon forecasts require structural components (local trends, cycles).</li> <li>Nonlinear effects are material and you have data/compute for richer models.</li> </ul>"},{"location":"arima/#12-takeaway","title":"12) Takeaway","text":"<p>ARIMA is a compact, transparent way to model short-run linear mean dynamics once low-frequency structure is removed. It shines as a baseline and in pipelines where interpretability and reliability matter. Its main gaps\u2014seasonality, exogenous drivers, nonlinearities, and time-varying variance\u2014are well understood and addressable via targeted extensions (SARIMA, ARIMAX, GARCH, state-space).</p>"},{"location":"introduction/","title":"Introduction","text":""},{"location":"introduction/#introduction-theoretical-fundamentals","title":"Introduction \u2014 Theoretical Fundamentals","text":"<p>This chapter lays the theoretical groundwork that underpins AutoArimaExplorer. It focuses on why we model, what ARIMA models are, how we select/diagnose them, and how we evaluate forecasts\u2014especially under the practical constraints typical of financial time series (short memory, heavy tails, and time\u2010varying volatility).</p>"},{"location":"introduction/#1-the-modeling-goal","title":"1) The modeling goal","text":"<p>Given a univariate time series ${y_t}_{t=1}^T$, we want to:</p> <ol> <li>Explain short-run serial dependence parsimoniously.</li> <li>Forecast $y_{T+h}$ (and uncertainty around it) with stable, out-of-sample performance.</li> <li>Diagnose whether remaining structure in the residuals violates standard modeling assumptions (e.g., autocorrelation).</li> </ol> <p>AutoArimaExplorer automates this pipeline with robust fitting strategies, information-criterion selection, residual diagnostics, and rolling-origin validation.</p>"},{"location":"introduction/#2-arima-models-at-a-glance","title":"2) ARIMA models at a glance","text":"<p>An ARIMA(p, d, q) model applies $d$ differences to make the series \u201cclose enough\u201d to stationary, then fits an ARMA(p, q) to the differenced series.</p> <ul> <li>Differencing:</li> </ul> <p>$$   \\Delta^d y_t = (1 - B)^d y_t   $$</p> <p>where $B$ is the backshift operator, $By_t = y_{t-1}$.</p> <ul> <li>ARMA(p, q) on $x_t = \\Delta^d y_t$:</li> </ul> <p>$$   \\phi(B)\\, x_t = \\theta(B)\\, \\varepsilon_t,\\quad \\varepsilon_t \\sim \\text{i.i.d. }(0, \\sigma^2)   $$</p> <p>with $\\phi(B) = 1 - \\phi_1B - \\dots - \\phi_p B^p$ and $\\theta(B) = 1 + \\theta_1B + \\dots + \\theta_q B^q$.</p> <p>Interpretation</p> <ul> <li>$p$: autoregressive memory (how many past values of $x_t$ matter).</li> <li>$d$: differencing order (how much trend we remove).</li> <li>$q$: moving-average memory (how many past shocks matter).</li> </ul> <p>Practical note in finance Prices are often modeled via log-returns (already close to stationary), for which $d=0$ is common; levels may require $d\\ge 1$.</p>"},{"location":"introduction/#3-stationarity-differencing-and-trend","title":"3) Stationarity, differencing, and trend","text":"<ul> <li>Weak stationarity (constant mean/variance, ACF depends only on lag) is a working assumption for ARMA modeling.</li> <li>Differencing helps when trends or unit roots are present. Over-differencing injects moving-average structure and can inflate forecast variance.</li> <li> <p>Trend terms: In integrated models, trend terms of order lower than $d$ are redundant (e.g., a constant is eliminated by first differencing). AutoArimaExplorer enforces sensible trend choices by $d$:</p> </li> <li> <p>$d=0$: allow constant (\u201cc\u201d) or no constant (\u201cn\u201d).</p> </li> <li>$d=1$: allow drift (\u201ct\u201d) or none (\u201cn\u201d).</li> <li>$d\\ge 2$: no explicit trend (lower-order trends are differenced out).</li> </ul>"},{"location":"introduction/#4-identification-and-parsimony","title":"4) Identification and parsimony","text":"<p>Classical Box\u2013Jenkins identification uses the ACF/PACF to suggest $p, q$, but automated search commonly relies on information criteria:</p> <ul> <li>BIC (Bayesian Information Criterion):</li> </ul> <p>$$   \\text{BIC} = -2\\log L + k \\log T   $$</p> <p>with $k$ number of estimated parameters; smaller is better.   BIC penalizes complexity more strongly than AIC, encouraging parsimonious models that typically forecast more robustly in practice.</p> <p>AutoArimaExplorer:</p> <ul> <li>Fits a grid of $(p,d,q)$ with robust optimizers and constraints toggled.</li> <li>Selects by minimum BIC, optionally breaking ties by parsimony (smaller $p+q$).</li> </ul>"},{"location":"introduction/#5-residual-diagnostics-guardrails","title":"5) Residual diagnostics (guardrails)","text":"<p>A fitted model should leave white-noise residuals:</p> <ul> <li>No linear autocorrelation in $\\varepsilon_t$.</li> <li>(Often in finance) Variance can be time-varying; ARIMA won\u2019t fix conditional heteroskedasticity.</li> </ul> <p>We use Ljung\u2013Box tests as guardrails:</p> <ul> <li>On residuals $\\varepsilon_t$ \u2192 checks leftover linear dependence.</li> <li>Optionally on squared residuals $\\varepsilon_t^2$ \u2192 crude check for short-run volatility clustering (ARCH-type effects).</li> </ul> <p>Decision rule: fail (reject white noise) if any p-value at selected lags is below $\\alpha$. Models that fail can be filtered out during selection.</p>"},{"location":"introduction/#6-convergence-and-stability","title":"6) Convergence and stability","text":"<p>Likelihood-based ARIMA fitting can struggle when:</p> <ul> <li>Roots are near the unit circle (non-stationary/invertible starts).</li> <li>The likelihood surface is flat or multi-modal.</li> <li>The sample is short relative to $p+q+d$.</li> </ul> <p>AutoArimaExplorer mitigates this by:</p> <ul> <li> <p>Trying multiple estimation strategies per candidate:</p> </li> <li> <p>For $d=0$, innovations MLE (fast, stable) and state-space with L-BFGS.</p> </li> <li>For $d\\ge 1$, state-space fits with/without stationarity/invertibility enforcement.</li> <li>Discarding pathological fits (non-finite BIC).</li> <li>Optionally requiring convergence.</li> </ul>"},{"location":"introduction/#7-how-much-data-do-we-need","title":"7) How much data do we need?","text":"<p>A pragmatic heuristic (Box\u2013Jenkins-style):</p> <p>$$ T - d \\;\\ge\\; \\max\\big( \\text{base},\\; \\text{per_param}\\cdot(p+q+d+1)\\big), $$</p> <p>with typical defaults like base = 50, per_param = 10. This avoids overfitting high-order models on very short samples.</p>"},{"location":"introduction/#8-out-of-sample-oos-validation","title":"8) Out-of-sample (OOS) validation","text":"<p>In-sample fit is not enough. We assess forecast performance via rolling-origin validation:</p> <ol> <li>Choose a minimal training window.</li> <li>For each fold $t$: fit on $[1..t]$, forecast $h$ steps ahead, compare to actuals.</li> <li>Aggregate errors across folds.</li> </ol> <p>Metrics</p> <ul> <li>RMSE: root mean squared error (penalizes large misses).</li> <li>MAE: mean absolute error (robust to outliers vs RMSE).</li> <li>MAPE: mean absolute percentage error (scale-free; beware near zero).</li> <li>Winkler score (for prediction intervals): balances interval width and coverage; lower is better.</li> </ul> <p>AutoArimaExplorer can select the model with lowest OOS score, complementing BIC-based selection.</p>"},{"location":"introduction/#9-prediction-intervals-pis-and-winkler-score","title":"9) Prediction intervals (PIs) and Winkler score","text":"<p>For a nominal level $1-\\alpha$ (e.g., 95%), a PI $[L_t, U_t]$ should contain the realization with probability $1-\\alpha$. The Winkler score penalizes both width and misses:</p> <p>$$ W = \\begin{cases} U - L, &amp; \\text{if } y \\in [L, U] \\ (U - L) + \\tfrac{2}{\\alpha}(L - y), &amp; \\text{if } y &lt; L \\ (U - L) + \\tfrac{2}{\\alpha}(y - U), &amp; \\text{if } y &gt; U \\ \\end{cases} $$</p> <p>Averaging $W$ across folds gives a scalar measure of sharpness + calibration.</p>"},{"location":"introduction/#10-when-arima-isnt-enough","title":"10) When ARIMA isn\u2019t enough","text":"<p>ARIMA captures linear serial dependence in the mean. It does not model:</p> <ul> <li>Conditional heteroskedasticity (GARCH-type effects).</li> <li>Nonlinear dynamics (thresholds, regime switches).</li> <li>Seasonality (unless explicitly modeled via SARIMA).</li> </ul> <p>If Ljung\u2013Box on squared residuals fails or volatility clustering is clear, consider ARIMA + GARCH, or robust forecasting of returns with a separate volatility model.</p>"},{"location":"introduction/#11-notation-used-throughout","title":"11) Notation used throughout","text":"<ul> <li>$y_t$: original series (levels or log-levels).</li> <li>$x_t = \\Delta^d y_t$: differenced (working) series.</li> <li>$p, d, q$: AR, integration, MA orders.</li> <li>$\\varepsilon_t$: one-step-ahead residuals/innovations.</li> <li>$h$: forecast horizon; $T$: sample size.</li> </ul>"},{"location":"introduction/#12-how-this-theory-informs-autoarimaexplorer","title":"12) How this theory informs AutoArimaExplorer","text":"<ul> <li>Robust fitting across trends/constraints reflects stationarity/invertibility and estimation stability concerns.</li> <li>BIC-first selection embodies parsimony; Ljung\u2013Box guardrails enforce whiteness.</li> <li>Rolling OOS closes the loop on predictive validity.</li> <li>Data sufficiency checks avoid overfitting with short samples.</li> </ul>"},{"location":"statistichal_conceps/","title":"Statistichal conceps","text":"<p>awesome\u2014here\u2019s a deep, math-forward chapter of the statistical and mathematical concepts you need to fully understand the tool we built. It\u2019s written as a standalone \u201ctheory\u201d section you can drop into your docs.</p>"},{"location":"statistichal_conceps/#statistical-mathematical-concepts-for-arima-modeling","title":"Statistical &amp; Mathematical Concepts for ARIMA Modeling","text":"<p>This chapter covers the core probability/stats background behind ARIMA-type models and the diagnostics/criteria we use in the tool. It includes definitions, assumptions, identification conditions, key tests, estimation methods, and forecasting math.</p>"},{"location":"statistichal_conceps/#1-time-series-basics","title":"1) Time Series Basics","text":""},{"location":"statistichal_conceps/#11-stochastic-processes-and-indexing","title":"1.1 Stochastic Processes and Indexing","text":"<p>A (univariate) time series is a sequence ${Y_t}_{t\\in\\mathbb{Z}}$ of random variables indexed by discrete time $t$. We observe a single realization $y_1,\\dots,y_T$.</p> <p>The backshift/lag operator $B$ is defined by $B Y_t := Y_{t-1}$. Powers: $B^k Y_t = Y_{t-k}$.</p>"},{"location":"statistichal_conceps/#12-moments-autocovariance-and-acf","title":"1.2 Moments, Autocovariance, and ACF","text":"<ul> <li>Mean: $\\mu_t = \\mathbb{E}[Y_t]$</li> <li>Autocovariance at lag $h$: $\\gamma_Y(h) = \\mathrm{Cov}(Y_t, Y_{t-h})$</li> <li>Autocorrelation function (ACF): $\\rho_Y(h) = \\gamma_Y(h)/\\gamma_Y(0)$</li> </ul>"},{"location":"statistichal_conceps/#13-stationarity","title":"1.3 Stationarity","text":"<ul> <li>Strict stationarity: the joint distribution of $(Y_{t_1},\\dots,Y_{t_k})$ is invariant to time shifts.</li> <li>Weak (covariance) stationarity:</li> </ul> <p>$$   \\mathbb{E}[Y_t]=\\mu\\ \\text{(constant)},\\quad \\mathrm{Var}(Y_t)=\\sigma^2\\ \\text{(finite, constant)},\\quad \\gamma_Y(h)\\ \\text{depends only on}\\ h.   $$</p> <p>ARMA modeling assumes weak stationarity (after appropriate transformations/differencing).</p>"},{"location":"statistichal_conceps/#14-wold-decomposition","title":"1.4 Wold Decomposition","text":"<p>Every zero-mean, weakly stationary process admits the Wold representation:</p> <p>$$ Y_t = \\sum_{j=0}^{\\infty}\\psi_j \\varepsilon_{t-j},\\quad \\sum_{j=0}^\\infty \\psi_j^2&lt;\\infty, $$</p> <p>with ${\\varepsilon_t}$ a white noise (WN) process: $\\varepsilon_t\\overset{iid}{\\sim}(0,\\sigma_\\varepsilon^2)$. ARMA models are finite-order parametric approximations of this representation.</p>"},{"location":"statistichal_conceps/#2-ar-ma-arma-models","title":"2) AR, MA, ARMA Models","text":""},{"location":"statistichal_conceps/#21-arp","title":"2.1 AR(p)","text":"<p>$$ \\phi(B)Y_t = \\varepsilon_t,\\quad \\phi(B)=1-\\phi_1 B - \\dots - \\phi_p B^p. $$</p> <p>Stationarity condition: all roots of $\\phi(z)=0$ lie outside the unit circle ($|z|&gt;1$).</p>"},{"location":"statistichal_conceps/#22-maq","title":"2.2 MA(q)","text":"<p>$$ Y_t = \\theta(B)\\varepsilon_t,\\quad \\theta(B)=1+\\theta_1 B+\\dots+\\theta_q B^q. $$</p> <p>Invertibility condition: all roots of $\\theta(z)=0$ lie outside the unit circle (ensures a unique Wold representation).</p>"},{"location":"statistichal_conceps/#23-armapq","title":"2.3 ARMA(p,q)","text":"<p>$$ \\phi(B)Y_t = \\theta(B)\\varepsilon_t. $$</p> <p>Combine the AR stationarity and MA invertibility conditions. The ACF/PACF patterns are classical identification heuristics:</p> <ul> <li>AR(p): PACF cuts off at lag $p$, ACF tails off.</li> <li>MA(q): ACF cuts off at lag $q$, PACF tails off.</li> <li>ARMA(p,q): both tail off.</li> </ul>"},{"location":"statistichal_conceps/#3-arima-integrated-arma","title":"3) ARIMA (Integrated ARMA)","text":""},{"location":"statistichal_conceps/#31-differencing","title":"3.1 Differencing","text":"<p>To remove low-frequency nonstationarity, difference $d$ times:</p> <p>$$ \\nabla^d Y_t := (1-B)^d Y_t = \\sum_{k=0}^d \\binom{d}{k}(-1)^k Y_{t-k}. $$</p> <p>Then model $X_t := \\nabla^d Y_t$ as ARMA(p,q):</p> <p>$$ \\phi(B) X_t = \\theta(B)\\varepsilon_t \\quad \\Longleftrightarrow \\quad \\phi(B)(1-B)^d Y_t = \\theta(B)\\varepsilon_t. $$</p>"},{"location":"statistichal_conceps/#32-drifttrend-interpretation","title":"3.2 Drift/Trend Interpretation","text":"<p>Including a constant in the model for $X_t=\\nabla^d Y_t$ corresponds to a polynomial trend of degree $d-1$ in levels $Y_t$ (e.g., with $d=1$, a constant \u21d2 linear trend in $Y_t$).</p>"},{"location":"statistichal_conceps/#4-identification-pre-testing","title":"4) Identification &amp; Pre-Testing","text":""},{"location":"statistichal_conceps/#41-unit-root-tests-d","title":"4.1 Unit Root Tests (d)","text":"<ul> <li>ADF (Augmented Dickey\u2013Fuller) test:   Tests $H_0:$ unit root (non-stationary) vs $H_1:$ stationary. Regression (with optional constant/trend):</li> </ul> <p>$$   \\Delta Y_t = \\alpha + \\beta t + \\gamma Y_{t-1} + \\sum_{i=1}^k \\delta_i \\Delta Y_{t-i} + u_t.   $$</p> <p>Rejecting $H_0:\\gamma=0$ suggests stationarity (no unit root).</p> <ul> <li> <p>KPSS test:   Tests $H_0:$ stationary around (possibly) deterministic trend vs $H_1:$ unit root. Complementary to ADF.</p> </li> <li> <p>PP (Phillips\u2013Perron):   Non-parametric correction for serial correlation and heteroskedasticity.</p> </li> </ul> <p>Practice: choose the smallest $d$ that renders the series \u201capproximately stationary\u201d (over-differencing inflates noise and induces spurious MA dynamics).</p>"},{"location":"statistichal_conceps/#42-transformations-for-variance-stabilization","title":"4.2 Transformations for Variance Stabilization","text":"<ul> <li>Log transform: $X_t=\\log(Y_t + c)$ for $Y_t&gt; -c$.</li> <li>Box\u2013Cox: $X_t=\\frac{Y_t^\\lambda-1}{\\lambda}$ for $\\lambda\\neq 0$ (and $ \\log Y_t$ if $\\lambda=0$).</li> </ul>"},{"location":"statistichal_conceps/#5-estimation","title":"5) Estimation","text":""},{"location":"statistichal_conceps/#51-likelihood-exactconditionalstate-space","title":"5.1 Likelihood (Exact/Conditional/State-Space)","text":"<p>Let $X_t=\\nabla^d Y_t$.</p> <ul> <li>Conditional Sum of Squares (CSS): minimize $\\sum_{t=m+1}^T \\hat\\varepsilon_t^2(\\theta)$ given initial conditions. Fast but approximate.</li> <li>Exact MLE: under Gaussian errors, maximize the Gaussian likelihood using the exact ARIMA covariance; slower but efficient asymptotically.</li> <li>State-space MLE (Kalman filter): represent ARIMA as a linear Gaussian state-space model; evaluate likelihood with Kalman recursions; robust and flexible (works with missing values, time-varying settings).</li> <li>Innovations MLE: evaluates the likelihood via innovations algorithm using the implied MA($\\infty$) representation.</li> </ul> <p>Numerics. Quasi-Newton optimizers like L-BFGS are common. Near-unit roots, high orders, and AR/MA near-cancellation can cause ill-conditioning and non-convergence. Enforcing stationarity/invertibility during optimization stabilizes estimation.</p>"},{"location":"statistichal_conceps/#6-model-selection-criteria","title":"6) Model Selection Criteria","text":"<p>Let $k$ be the number of estimated parameters (including variance), $\\hat L$ the maximized likelihood.</p> <ul> <li>AIC: $\\mathrm{AIC} = -2\\log \\hat L + 2k$</li> <li>AICc: small-sample corrected AIC</li> </ul> <p>$$   \\mathrm{AICc} = \\mathrm{AIC} + \\frac{2k(k+1)}{T - k - 1}.   $$ * BIC (Schwarz): $\\mathrm{BIC} = -2\\log \\hat L + k\\log T$</p> <p>Lower is better. BIC penalizes complexity more strongly; it tends to pick more parsimonious models (our default selector). A useful rule of thumb: $\\Delta \\mathrm{BIC} \\gtrsim 2$ indicates meaningful improvement.</p>"},{"location":"statistichal_conceps/#7-diagnostics-residual-whiteness-heteroskedasticity","title":"7) Diagnostics: Residual Whiteness &amp; Heteroskedasticity","text":""},{"location":"statistichal_conceps/#71-portmanteau-tests-ljungbox","title":"7.1 Portmanteau Tests (Ljung\u2013Box)","text":"<p>After fitting, residuals $\\hat\\varepsilon_t$ should be serially uncorrelated. Define residual autocorrelations $\\hat\\rho(h)$ for $h=1,\\dots,m$.</p> <ul> <li>Ljung\u2013Box statistic:</li> </ul> <p>$$   Q(m) = T(T+2)\\sum_{h=1}^m \\frac{\\hat\\rho(h)^2}{T-h}.   $$</p> <p>Under the null of no autocorrelation up to lag $m$, $Q(m)$ is approximately $\\chi^2_{m-k}$ (degrees of freedom adjusted for estimated parameters $k$). Large $Q$ \u21d2 small $p$-value \u21d2 residual autocorrelation remains.</p> <p>We also apply Ljung\u2013Box to squared residuals $\\hat\\varepsilon_t^2$ to screen for conditional heteroskedasticity (ARCH effects).</p>"},{"location":"statistichal_conceps/#72-engles-arch-lm-test","title":"7.2 Engle\u2019s ARCH LM Test","text":"<p>Regress $\\hat\\varepsilon_t^2$ on its own lags:</p> <p>$$ \\hat\\varepsilon_t^2 = \\alpha_0 + \\sum_{i=1}^m \\alpha_i \\hat\\varepsilon_{t-i}^2 + u_t. $$</p> <p>Test $H_0: \\alpha_1=\\dots=\\alpha_m=0$. A significant test suggests time-varying variance; consider ARIMA+GARCH.</p>"},{"location":"statistichal_conceps/#8-forecasting-mathematics","title":"8) Forecasting Mathematics","text":""},{"location":"statistichal_conceps/#81-h-step-forecasts","title":"8.1 h-step Forecasts","text":"<p>For a stationary ARMA on $X_t$ with Wold coefficients ${\\psi_j}$:</p> <p>$$ X_{t+h|t} = \\mathbb{E}[X_{t+h}\\mid \\mathcal{F}t] = \\sum{j=h}^{\\infty}\\psi_j \\varepsilon_{t+h-j}. $$</p> <p>Practically, AR recursions + estimated residuals yield forecasts. For ARIMA, forecasts are made on $X_t=\\nabla^d Y_t$ and then integrated back by summation to obtain $Y_{t+h|t}$.</p>"},{"location":"statistichal_conceps/#82-forecast-error-variance-intervals","title":"8.2 Forecast Error Variance &amp; Intervals","text":"<p>Let $\\sigma_\\varepsilon^2$ be innovation variance and $\\psi_j$ the MA($\\infty$) coefficients of the fitted ARMA for $X_t$. The $h$-step forecast error variance for $X_t$ is</p> <p>$$ \\mathrm{Var}\\big(X_{t+h}-X_{t+h|t}\\big) = \\sigma_\\varepsilon^2 \\sum_{j=0}^{h-1}\\psi_j^2. $$</p> <p>For $Y_t$ (when $d&gt;0$), the variance accumulates due to integration.</p> <p>Assuming Gaussian errors, (1-$\\alpha$) prediction intervals for $X_{t+h}$ are:</p> <p>$$ \\hat X_{t+h|t} \\pm z_{1-\\alpha/2}\\,\\hat\\sigma_\\varepsilon \\sqrt{\\sum_{j=0}^{h-1}\\hat\\psi_j^2}. $$</p> <p>Intervals for $Y_{t+h}$ follow by integration. In practice, software computes these via state-space or equivalent formulas.</p>"},{"location":"statistichal_conceps/#83-winkler-score-interval-quality","title":"8.3 Winkler Score (Interval Quality)","text":"<p>For a (1-$\\alpha$) PI $[L,U]$ and realized value $y$:</p> <p>$$ \\text{Winkler}_\\alpha = (U-L) + \\frac{2}{\\alpha}\\,(L-y)\\,\\mathbf{1}{yU}. $$ <p>Lower is better; it rewards narrow intervals with correct coverage and penalizes misses.</p>"},{"location":"statistichal_conceps/#9-time-series-cross-validation-rolling-origin","title":"9) Time-Series Cross-Validation (Rolling Origin)","text":"<p>Given a series $y_1,\\dots,y_T$, horizon $h$, and initial training size $n_0$:</p> <p>For folds $t=n_0, n_0+\\text{step}, \\dots, T-h$:</p> <ol> <li>Fit on $y_{1:t}$.</li> <li>Forecast $h$ steps: $\\hat y_{t+1|t},\\dots,\\hat y_{t+h|t}$.</li> <li>Compute error(s) vs $y_{t+1},\\dots,y_{t+h}$.</li> </ol> <p>Aggregate per model:</p> <ul> <li>RMSE: $\\sqrt{\\frac{1}{N}\\sum (y-\\hat y)^2}$</li> <li>MAE: $\\frac{1}{N}\\sum |y-\\hat y|$</li> <li>MAPE: $\\frac{100}{N}\\sum \\left|\\frac{y-\\hat y}{y}\\right|$ (use epsilon for small $y$)</li> <li>Winkler score: average across horizons/folds.</li> </ul> <p>This procedure respects time order and uses only information available at the forecast origin.</p>"},{"location":"statistichal_conceps/#10-numerical-identifiability-issues","title":"10) Numerical &amp; Identifiability Issues","text":""},{"location":"statistichal_conceps/#101-stationarityinvertibility-constraints","title":"10.1 Stationarity/Invertibility Constraints","text":"<ul> <li>Enforcing roots outside the unit circle stabilizes estimation and ensures a valid infinite-order representation.</li> <li>Relaxing constraints can improve local fit but risks degenerate/near-non-invertible solutions and poor forecast behavior.</li> </ul>"},{"location":"statistichal_conceps/#102-near-cancellations","title":"10.2 Near Cancellations","text":"<p>AR and MA polynomials can nearly cancel (e.g., $\\phi(B)\\approx \\theta(B)$), creating flat likelihood surfaces and unstable parameters.</p>"},{"location":"statistichal_conceps/#103-optimization-warnings","title":"10.3 Optimization Warnings","text":"<ul> <li>Non-stationary starting AR parameters: optimizer resets to zeros or constrained region.</li> <li>Non-invertible starting MA parameters: same idea on the MA side.</li> <li>Failed convergence: increase iterations, switch optimizer, simplify model, or re-scale.</li> </ul>"},{"location":"statistichal_conceps/#11-residual-analysis-in-practice","title":"11) Residual Analysis in Practice","text":"<ul> <li>Whiteness in mean: residual ACF within bands; Ljung\u2013Box $p$-values \u201cnot small\u201d at chosen lags.</li> <li>Variance dynamics: ACF of squared residuals; Ljung\u2013Box on $\\hat\\varepsilon_t^2$; ARCH LM test.</li> <li>Distributional shape: heavy tails/asymmetry \u2192 consider robust intervals, bootstrap, or volatility modeling.</li> </ul> <p>Passing mean-whiteness does not imply calibrated uncertainty if variance is time-varying.</p>"},{"location":"statistichal_conceps/#12-scaling-and-transformations-used-in-the-tool","title":"12) Scaling and Transformations Used in the Tool","text":"<ul> <li>Min\u2013Max scaling (for OOS comparability across different $d$ choices):</li> </ul> <p>$$   x't = \\frac{x_t - \\min(x)}{\\max(x) - \\min(x) + \\varepsilon}.   $$ * Log-returns for $d=0$ ARMA (financial data):   Given levels $P_t$, we work with $r_t = \\log(P_t+c) - \\log(P{t-1}+c)$ after ensuring positivity ($c&gt;0$ if needed) and optionally standardize to unit scale.</p>"},{"location":"statistichal_conceps/#13-why-bic-ljungbox-guardrails","title":"13) Why BIC + Ljung\u2013Box Guardrails?","text":"<ul> <li>BIC: favors parsimonious models with genuine likelihood gains; reduces overfitting.</li> <li>Ljung\u2013Box guardrail: ensures selected models actually remove linear dependence in residuals (our minimum standard for \u201cadequate\u201d mean dynamics).</li> <li>Optional squared-residual check adds a quick screen for short-run heteroskedasticity (flagging when ARIMA alone is insufficient).</li> </ul>"},{"location":"statistichal_conceps/#14-summary-of-notation","title":"14) Summary of Notation","text":"<ul> <li>$Y_t$: original series (level)</li> <li>$X_t=\\nabla^d Y_t$: differenced series</li> <li>$B$: backshift operator; $BY_t=Y_{t-1}$</li> <li>$\\phi(B)$: AR polynomial; $\\theta(B)$: MA polynomial</li> <li>$\\varepsilon_t$: innovations (white noise)</li> <li>$\\psi_j$: Wold coefficients (MA$(\\infty)$)</li> <li>$\\rho(h)$: autocorrelation at lag $h$</li> <li>$k$: number of parameters</li> <li>AIC, BIC: information criteria</li> <li>$Q(m)$: Ljung\u2013Box statistic at lag $m$</li> </ul>"},{"location":"statistichal_conceps/#15-where-each-concept-lives-in-the-tool","title":"15) Where Each Concept Lives in the Tool","text":"<ul> <li>Stationarity &amp; differencing (Section 3) \u2192 choice of $d$; using returns for $d=0$, levels for $d\\ge1$.</li> <li>AR/MA orders &amp; constraints (Section 2, 10) \u2192 grid exploration and constrained optimization.</li> <li>Likelihood and BIC (Sections 5\u20136) \u2192 estimation and primary selection criterion.</li> <li>Ljung\u2013Box tests (Section 7) \u2192 post-fit guardrails in selection functions.</li> <li>OOS validation (Section 9) \u2192 rolling origin selection route.</li> <li>Variance checks (Sections 7, 11) \u2192 optional squared-residual diagnostics; suggest ARIMA+GARCH if needed.</li> </ul>"},{"location":"statistichal_conceps/#further-reading-short-list","title":"Further Reading (short list)","text":"<ul> <li>Box, Jenkins, Reinsel &amp; Ljung \u2014 Time Series Analysis: Forecasting and Control</li> <li>Brockwell &amp; Davis \u2014 Introduction to Time Series and Forecasting</li> <li>Hyndman &amp; Athanasopoulos \u2014 Forecasting: Principles and Practice (free online)</li> </ul> <p>If you want, I can turn this into a <code>statistical-concepts.md</code> file with a clean table of contents and cross-links to your \u201cARIMA implementation\u201d and \u201cTutorials\u201d chapters.</p>"}]}